"""
Ù…Ù‡Ø§Ù… Celery Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù„Ù…Ø®Ø²ÙˆÙ† - Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ ÙŠÙ…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
"""

import logging
from decimal import Decimal
from io import BytesIO

import pandas as pd
from celery import shared_task
from django.db import transaction
from django.utils import timezone

logger = logging.getLogger(__name__)


@shared_task(bind=True, time_limit=600, soft_time_limit=540, rate_limit=None)
def bulk_upload_products_fast(
    self,
    upload_log_id,
    file_content,
    warehouse_id,
    upload_mode,
    user_id,
    auto_delete_empty=False,
):
    """
    Ø±ÙØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© - Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ Ù…Ø­Ø³Ù‘Ù†
    """
    from django.contrib.auth import get_user_model

    from .models import (
        BaseProduct,
        BulkUploadError,
        BulkUploadLog,
        Category,
        Product,
        ProductVariant,
        StockTransaction,
        Warehouse,
    )
    from .smart_upload_logic import (
        add_stock_transaction,
        clean_start_reset,
        delete_empty_warehouses,
        smart_update_product,
    )

    User = get_user_model()
    logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø±ÙØ¹ Ø§Ù„Ø°ÙƒÙŠ - Log: {upload_log_id} - Ø§Ù„ÙˆØ¶Ø¹: {upload_mode}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù„ÙˆØ¬ Ù„Ù„ØªØªØ¨Ø¹
    import os
    from django.conf import settings
    
    log_dir = os.path.join(settings.BASE_DIR, 'logs')
    os.makedirs(log_dir, exist_ok=True)
    log_file_path = os.path.join(log_dir, f'bulk_upload_{upload_log_id}.log')
    
    try:
        log_file = open(log_file_path, 'w', encoding='utf-8')
        
        def log_message(msg):
            """Ø­ÙØ¸ Ø±Ø³Ø§Ù„Ø© ÙÙŠ Ø§Ù„Ù…Ù„Ù ÙˆØ§Ù„Ø·Ø¨Ø§Ø¹Ø©"""
            logger.info(msg)
            log_file.write(msg + '\n')
            log_file.flush()
    except Exception as e:
        logger.error(f"ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„Ù„ÙˆØ¬: {e}")
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… logger ÙÙ‚Ø· Ø¥Ø°Ø§ ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù
        def log_message(msg):
            logger.info(msg)

    # ØªØ¹Ø·ÙŠÙ„ Cloudflare signals Ù„Ù…Ù†Ø¹ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    from django.db.models.signals import post_save, pre_save

    from .models import BaseProduct, ProductVariant

    # Ø­ÙØ¸ receivers Ø§Ù„Ø£ØµÙ„ÙŠØ©
    original_post_save = list(post_save.receivers)
    original_pre_save = list(pre_save.receivers)

    # ØªØ¹Ø·ÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù€ signals
    post_save.receivers = []
    pre_save.receivers = []
    log_message("âš¡ ØªÙ… ØªØ¹Ø·ÙŠÙ„ Signals Ù„Ù„Ø³Ø±Ø¹Ø©")

    try:
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        upload_log = BulkUploadLog.objects.get(id=upload_log_id)
        user = User.objects.get(id=user_id)
        warehouse = Warehouse.objects.get(id=warehouse_id) if warehouse_id else None

        upload_log.status = "processing"
        upload_log.save(update_fields=["status"])

        log_message(f"{'='*80}")
        log_message(f"ğŸ“ Ø³Ø¬Ù„ Ø±ÙØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª - ID: {upload_log_id}")
        log_message(f"{'='*80}")
        log_message(f"ğŸ“ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù: {upload_log.file_name}")
        log_message(f"ğŸ¢ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹: {warehouse.name if warehouse else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}")
        log_message(f"â™»ï¸ ÙˆØ¶Ø¹ Ø§Ù„Ø±ÙØ¹: {upload_mode}")
        log_message(f"ğŸ‘¤ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user.username}")
        log_message(f"{'='*80}\n")

        # Ù‚Ø±Ø§Ø¡Ø© Excel Ø¨Ø³Ø±Ø¹Ø©
        log_message("ğŸ“Š Ù‚Ø±Ø§Ø¡Ø© Excel...")
        df = pd.read_excel(BytesIO(file_content), engine="openpyxl")
        total = len(df)
        # ØªØ­Ø¯ÙŠØ« total_rows Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ù„Ø¶Ù…Ø§Ù† Ø¸Ù‡ÙˆØ± Ø§Ù„Ù€ 0% ÙƒØ¨Ø¯Ø§ÙŠØ© Ø­Ù‚ÙŠÙ‚ÙŠØ©
        upload_log.total_rows = total
        upload_log.processed_count = 0
        upload_log.status = "processing"
        upload_log.save(update_fields=["total_rows", "processed_count", "status"])

        log_message(f"ğŸ“‹ {total} ØµÙ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
        log_message(f"ğŸ“ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù„Ù: {list(df.columns)}\n")

        # ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ù…Ù„Ø© Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨
        if upload_mode == "clean_start":
            logger.warning("âš ï¸ ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ù…Ù„")
            reset_stats = clean_start_reset()
            upload_log.summary = f"Ù…Ø³Ø­ ÙƒØ§Ù…Ù„: {reset_stats['deleted_products']} Ù…Ù†ØªØ¬ØŒ {reset_stats['deleted_transactions']} Ù…Ø¹Ø§Ù…Ù„Ø©"
            upload_log.save(update_fields=["summary"])

        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„Ù…Ø³Ø­
        subset_cols = []
        name_cols = ["Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬", "Ø§Ù„Ø§Ø³Ù…", "product_name", "name"]
        code_cols = ["Ø§Ù„ÙƒÙˆØ¯", "ÙƒÙˆØ¯ Ø§Ù„Ù…Ù†ØªØ¬", "product_code", "code"]

        for col in name_cols + code_cols:
            if col in df.columns:
                subset_cols.append(col)

        if subset_cols:
            df = df.dropna(how="all", subset=subset_cols).fillna("")
        else:
            df = df.dropna(how="all").fillna("")

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¨Ø­Ø« (Caching)
        # Ù‡Ø°Ø§ ÙŠÙ‚Ù„Ù„ Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù† O(N) Ø¥Ù„Ù‰ O(1)
        logger.info("ğŸ§  Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        categories_cache = {c.name.strip(): c for c in Category.objects.all()}
        warehouses_cache = {
            w.name.strip(): w for w in Warehouse.objects.filter(is_active=True)
        }

        # ÙƒØ§Ø´ Ù„Ù„Ù…Ù†ØªØ¬Ø§Øª (Legacy Products) - Ù…Ø¹ ÙƒÙ„ Ù†Ø³Ø® Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        products_cache = {}
        for p in Product.objects.all():
            if p.code:
                products_cache[p.code] = p
                # Ø¥Ø¶Ø§ÙØ© Ù†Ø³Ø® Ø¨Ø£ØµÙØ§Ø± Ø¨Ø§Ø¯Ø¦Ø© Ù…Ø®ØªÙ„ÙØ© Ù„ØªØ¬Ù†Ø¨ IntegrityError
                if p.code.isdigit():
                    for padding in range(len(p.code), 15):
                        padded = p.code.zfill(padding)
                        if padded not in products_cache:
                            products_cache[padded] = p

        # ÙƒØ§Ø´ Ù„Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Base Products)
        base_products_cache = {
            bp.code: bp for bp in BaseProduct.objects.all() if bp.code
        }

        # ÙƒØ§Ø´ Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª (Variants) Ù…ÙÙ‡Ø±Ø³Ø© Ø¨Ù€ legacy_product_id
        variants_cache = {
            v.legacy_product_id: v
            for v in ProductVariant.objects.filter(
                legacy_product_id__isnull=False
            ).select_related("base_product")
        }

        data_cache = {
            "products": products_cache,
            "base_products": base_products_cache,
            "variants": variants_cache,
        }

        stats = {
            "created": 0,
            "updated": 0,
            "moved": 0,
            "merged": 0,
            "skipped": 0,
            "errors": 0,
            "cutting_updated": 0,
            "cutting_split": 0,
        }

        # ØªØªØ¨Ø¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…Ø¹Ø¯Ù„Ø©/Ø§Ù„Ù…Ù†Ø´Ø£Ø© Ù„Ø®Ø· Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
        affected_product_ids = []

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø¯ÙØ¹Ø§Øª Ù…ØªÙˆØ³Ø·Ø© Ù„Ù„ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø£Ù…Ø§Ù†
        batch_size = 50  # Ø¯ÙØ¹Ø§Øª Ø£ØµØºØ± Ù„ØªÙ‚Ù„ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        results_batch = []
        processed_overall = 0

        logger.info(f"ğŸ”„ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(df)} ØµÙ Ø¨Ø³Ø¹Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª ÙƒØ§Ù…Ù„Ø©")

        for batch_start in range(0, len(df), batch_size):
            batch_end = min(batch_start + batch_size, len(df))
            batch = df.iloc[batch_start:batch_end]

            logger.info(f"ğŸ“¦ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø© {batch_start}-{batch_end}")

            from django.db import transaction

            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙÙˆÙ - Ù…Ø¹Ø§Ù…Ù„Ø© Ù„ÙƒÙ„ 10 ØµÙÙˆÙ Ù„Ù„ØªÙˆØ§Ø²Ù†
            mini_batch_size = 10
            for mini_start in range(0, len(batch), mini_batch_size):
                mini_end = min(mini_start + mini_batch_size, len(batch))
                mini_batch = batch.iloc[mini_start:mini_end]

                try:
                    with transaction.atomic():
                        for i, (idx, row) in enumerate(mini_batch.iterrows()):
                            processed_overall += 1

                            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø±Ù‚Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ (Ù…Ù† Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù…ÙÙˆÙ„Ù‘Ø¯)
                            # 0=Ø§Ø³Ù…ØŒ 1=ÙƒÙˆØ¯ØŒ 2=ÙØ¦Ø©ØŒ 3=Ø³Ø¹Ø±ØŒ 4=Ø¬Ù…Ù„Ø©ØŒ 5=ÙƒÙ…ÙŠØ©ØŒ 6=Ù…Ø³ØªÙˆØ¯Ø¹ØŒ 7=ÙˆØµÙ

                            def safe_get(index, default=None):
                                """Ù‚Ø±Ø§Ø¡Ø© Ù‚ÙŠÙ…Ø© Ù…Ù† Ø¹Ù…ÙˆØ¯ Ø¨Ø±Ù‚Ù…Ù‡"""
                                if index < len(row):
                                    val = row.iloc[index]
                                    if pd.notna(val):
                                        val = str(val).strip()
                                        if val and val.lower() not in [
                                            "nan",
                                            "none",
                                            "",
                                        ]:
                                            return val
                                return default

                            # Ø§Ù„ÙƒÙˆØ¯ (Ø¹Ù…ÙˆØ¯ 1) - Ø§Ù„Ø£Ù‡Ù…
                            code = safe_get(1)
                            if code and code.isdigit():
                                code = code.lstrip("0") or "0"

                            # Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬ (Ø¹Ù…ÙˆØ¯ 0 Ø£Ùˆ Ø§Ù„Ø§Ø³Ù…)
                            name = row.get(
                                "Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬",
                                row.get(
                                    "Ø§Ù„Ø§Ø³Ù…", row.get("product_name", safe_get(0, ""))
                                ),
                            )

                            # Ø§Ù„Ø³Ø¹Ø± (Ø¹Ù…ÙˆØ¯ 3 Ø£Ùˆ Ø§Ù„Ø³Ø¹Ø±)
                            price_val = row.get(
                                "Ø§Ù„Ø³Ø¹Ø±", row.get("price", safe_get(3, None))
                            )
                            try:
                                if (
                                    price_val is not None
                                    and str(price_val).strip() != ""
                                ):
                                    price = float(price_val)
                                else:
                                    price = 0
                            except Exception:
                                price = 0

                            # Ø³Ø¹Ø± Ø§Ù„Ø¬Ù…Ù„Ø© - ØªØ¬Ø±Ø¨Ø© Ø¹Ø¯Ø© Ø£Ø¹Ù…Ø¯Ø© (4 Ø«Ù… 1 Ù„Ù„Ù…Ù„ÙØ§Øª Ø¨Ø¹Ù…ÙˆØ¯ÙŠÙ† Ø£Ùˆ Ø¨Ø§Ù„Ø§Ø³Ù…)
                            wholesale_price = row.get(
                                "Ø³Ø¹Ø± Ø§Ù„Ø¬Ù…Ù„Ø©", row.get("wholesale_price")
                            )
                            if pd.isna(wholesale_price) or wholesale_price == "":
                                for ws_col in [4, 1]:  # Ø§Ù„Ù‚Ø§Ù„Ø¨ = 4ØŒ Ù…Ù„Ù Ø¨Ø¹Ù…ÙˆØ¯ÙŠÙ† = 1
                                    try:
                                        ws_val = safe_get(ws_col)
                                        if ws_val:
                                            wholesale_price = float(ws_val)
                                            break
                                    except Exception:
                                        continue
                            try:
                                if wholesale_price is not None:
                                    wholesale_price = float(wholesale_price)
                            except Exception:
                                wholesale_price = None

                            # Ø§Ù„ÙƒÙ…ÙŠØ© (Ø¹Ù…ÙˆØ¯ 5 Ø£Ùˆ Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³Ù…)
                            quantity_val = row.get("Ø§Ù„ÙƒÙ…ÙŠØ©", safe_get(5))
                            try:
                                quantity = float(quantity_val) if quantity_val else 0
                            except Exception:
                                quantity = 0

                            # Ø§Ù„ÙˆØµÙ (Ø¹Ù…ÙˆØ¯ 7)
                            description = safe_get(7, "")
                            
                            # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ (Ø¹Ù…ÙˆØ¯ 8 Ø£Ùˆ Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³Ù…)
                            min_stock_val = row.get("Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰", safe_get(8))
                            try:
                                minimum_stock = int(float(min_stock_val)) if min_stock_val is not None else None
                            except Exception:
                                minimum_stock = None

                            # Material (Ø¹Ù…ÙˆØ¯ 9 Ø£Ùˆ Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³Ù…)
                            material = row.get(
                                "Material", row.get("Ø§Ù„Ø®Ø§Ù…Ø©", safe_get(9, None))
                            )
                            if pd.isna(material):
                                material = ""

                            # Width (Ø¹Ù…ÙˆØ¯ 10 Ø£Ùˆ Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³Ù…)
                            width = row.get(
                                "Width", row.get("Ø§Ù„Ø¹Ø±Ø¶", safe_get(10, None))
                            )
                            if pd.isna(width):
                                width = ""
                            
                            # Currency (Ø¹Ù…ÙˆØ¯ 11 Ø£Ùˆ Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³Ù…)
                            currency_val = row.get("Ø§Ù„Ø¹Ù…Ù„Ø©", safe_get(11))
                            currency = None
                            if currency_val and pd.notna(currency_val):
                                curr_str = str(currency_val).strip().upper()
                                if curr_str in ["EGP", "USD", "EUR", "SAR"]:
                                    currency = curr_str
                            
                            # Unit (Ø¹Ù…ÙˆØ¯ 12 Ø£Ùˆ Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³Ù…)
                            unit_val = row.get("Ø§Ù„ÙˆØ­Ø¯Ø©", safe_get(12))
                            unit = None
                            if unit_val and pd.notna(unit_val):
                                unit_str = str(unit_val).strip()
                                valid_units = ["piece", "kg", "gram", "liter", "meter", "box", "pack", "dozen", "roll", "sheet"]
                                unit_map = {
                                    "Ù‚Ø·Ø¹Ø©": "piece", "ÙƒÙŠÙ„ÙˆØ¬Ø±Ø§Ù…": "kg", "Ø¬Ø±Ø§Ù…": "gram",
                                    "Ù„ØªØ±": "liter", "Ù…ØªØ±": "meter", "Ø¹Ù„Ø¨Ø©": "box",
                                    "Ø¹Ø¨ÙˆØ©": "pack", "Ø¯Ø³ØªØ©": "dozen", "Ù„ÙØ©": "roll", "ÙˆØ±Ù‚Ø©": "sheet",
                                }
                                if unit_str in valid_units:
                                    unit = unit_str
                                elif unit_str in unit_map:
                                    unit = unit_map[unit_str]

                            # DEBUG: Ø·Ø¨Ø§Ø¹Ø© Ø£ÙˆÙ„ 3 ØµÙÙˆÙ
                            if processed_overall <= 3:
                                logger.warning(f"ğŸ” DEBUG ØµÙ {processed_overall}:")
                                logger.warning(f"   Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù„Ù: {list(row.index)}")
                                logger.warning(
                                    f"   ÙƒÙˆØ¯={code}, Ø³Ø¹Ø±={price}, Ø¬Ù…Ù„Ø©={wholesale_price}"
                                )

                            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ù†ØªØ¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒØ§Ø´
                            is_existing = False
                            actual_code = code
                            if code and "products" in data_cache:
                                if code in data_cache["products"]:
                                    is_existing = True
                                    actual_code = code
                                elif code.isdigit():
                                    for p in range(len(code), 15):
                                        padded = code.zfill(p)
                                        if padded in data_cache["products"]:
                                            is_existing = True
                                            actual_code = padded
                                            break

                            if not is_existing and not name:
                                stats["errors"] += 1
                                results_batch.append(
                                    BulkUploadError(
                                        upload_log=upload_log,
                                        row_number=idx + 2,
                                        error_type="missing_data",
                                        result_status="failed",
                                        error_message="Ù…Ù†ØªØ¬ Ø¬Ø¯ÙŠØ¯ ÙˆÙ„ÙƒÙ† Ø§Ù„Ø§Ø³Ù… Ù…ÙÙ‚ÙˆØ¯ - ÙŠØ±Ø¬Ù‰ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø§Ø³Ù… Ù„ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù† Ø¥Ù†Ø´Ø§Ø¦Ù‡",
                                        row_data={"code": actual_code},
                                    )
                                )
                                continue

                            # ØªØ¬Ù…ÙŠØ¹
                            product_data = {
                                "name": name,
                                "code": actual_code,
                                "price": price,
                                "wholesale_price": wholesale_price,
                                "quantity": quantity,
                                "material": material,
                                "width": width,
                                "description": description,
                                "minimum_stock": minimum_stock,
                                "currency": currency,
                                "unit": unit,
                            }

                            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙØ¦Ø© ÙˆØ§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ø§
                            cat_name = str(row.get("Ø§Ù„ÙØ¦Ø©", "")).strip()
                            if cat_name:
                                if cat_name in categories_cache:
                                    product_data["category"] = categories_cache[
                                        cat_name
                                    ]
                                else:
                                    cat = Category.objects.create(name=cat_name)
                                    categories_cache[cat_name] = cat
                                    product_data["category"] = cat

                            wh_name = str(row.get("Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹", "")).strip()
                            target_wh = warehouse
                            if wh_name:
                                if wh_name in warehouses_cache:
                                    target_wh = warehouses_cache[wh_name]
                                else:
                                    from .views_bulk import get_or_create_warehouse

                                    target_wh = get_or_create_warehouse(wh_name, user)
                                    if target_wh:
                                        warehouses_cache[wh_name] = target_wh

                            # Ø­ÙØ¸
                            result = smart_update_product(
                                product_data,
                                target_wh,
                                user,
                                upload_mode,
                                cache=data_cache,
                                fast_mode=True,
                            )

                            action = result["action"]
                            if action == "created":
                                stats["created"] += 1
                                if result.get("product"):
                                    affected_product_ids.append(result["product"].id)
                            elif action == "updated":
                                stats["updated"] += 1
                                if result.get("product"):
                                    affected_product_ids.append(result["product"].id)
                            elif action == "moved":
                                stats["moved"] += 1
                                stats["updated"] += 1
                                if result.get("product"):
                                    affected_product_ids.append(result["product"].id)
                            elif action == "skipped":
                                stats["skipped"] += 1

                            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„ØªÙ‚Ø±ÙŠØ± (Ø«Ù…Ù† Ù…Ø¹Ù†ÙˆÙŠ Ø¨Ø³ÙŠØ· Ù„Ù„Ø£Ø¯Ø§Ø¡ Ù…Ù‚Ø§Ø¨Ù„ Ø¯Ù‚Ø© Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±)
                            results_batch.append(
                                BulkUploadError(
                                    upload_log=upload_log,
                                    row_number=idx + 2,
                                    error_type="other",
                                    result_status=(
                                        action if action != "moved" else "updated"
                                    ),
                                    error_message=result.get("message", ""),
                                    row_data={
                                        "name": name or code or "Ø¨Ø¯ÙˆÙ† Ø§Ø³Ù…",
                                        "code": actual_code,
                                    },
                                )
                            )

                except Exception as mini_batch_error:
                    # ÙØ´Ù„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø© Ø§Ù„ØµØºÙŠØ±Ø© - ØªØ³Ø¬ÙŠÙ„ ÙƒÙ„ Ø§Ù„ØµÙÙˆÙ ÙƒØ£Ø®Ø·Ø§Ø¡
                    logger.error(f"âŒ ÙØ´Ù„ mini-batch: {mini_batch_error}")
                    for i, (idx, row) in enumerate(mini_batch.iterrows()):
                        if idx + 2 not in [r.row_number for r in results_batch]:
                            stats["errors"] += 1
                            results_batch.append(
                                BulkUploadError(
                                    upload_log=upload_log,
                                    row_number=idx + 2,
                                    error_type="processing",
                                    result_status="failed",
                                    error_message=str(mini_batch_error)[:500],
                                    row_data={},
                                )
                            )

            # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø¹Ø¯ ÙƒÙ„ Ø¯ÙØ¹Ø© ÙƒØ¨ÙŠØ±Ø©
            if results_batch:
                try:
                    BulkUploadError.objects.bulk_create(results_batch)
                    results_batch = []
                except Exception as save_error:
                    logger.error(f"âš ï¸ ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: {save_error}")

            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù… *Ø®Ø§Ø±Ø¬* Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø© Ù„Ø¶Ù…Ø§Ù† Ø¸Ù‡ÙˆØ±Ù‡ Ù„Ù„ÙˆØ§Ø¬Ù‡Ø© ÙÙˆØ±Ø§Ù‹
            percent = int((processed_overall / total) * 100)
            upload_log.processed_count = processed_overall
            upload_log.created_count = stats["created"]
            upload_log.updated_count = stats["updated"]
            upload_log.skipped_count = stats["skipped"]
            upload_log.error_count = stats["errors"]
            upload_log.save(
                update_fields=[
                    "processed_count",
                    "created_count",
                    "updated_count",
                    "skipped_count",
                    "error_count",
                ]
            )

            self.update_state(
                state="PROGRESS",
                meta={
                    "current": processed_overall,
                    "total": total,
                    "percent": percent,
                    "created": stats["created"],
                    "updated": stats["updated"],
                    "skipped": stats["skipped"],
                    "errors": stats["errors"],
                },
            )

        # Ø¥ÙƒÙ…Ø§Ù„
        log_message(f"\n{'='*80}")
        log_message(f"ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©:")
        log_message(f"   âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡: {stats['created']} Ù…Ù†ØªØ¬")
        log_message(f"   ğŸ”„ ØªÙ… ØªØ­Ø¯ÙŠØ«: {stats['updated']} Ù…Ù†ØªØ¬")
        log_message(f"   â­ï¸ ØªÙ… ØªØ®Ø·ÙŠ: {stats['skipped']} Ù…Ù†ØªØ¬")
        log_message(f"   âŒ Ø£Ø®Ø·Ø§Ø¡: {stats['errors']}")
        log_message(f"{'='*80}")
        log_message(f"\nğŸ“ Ù…Ù„Ù Ø§Ù„Ù„ÙˆØ¬ Ù…Ø­ÙÙˆØ¸ ÙÙŠ: {log_file_path}")
        log_message(f"ÙŠÙ…ÙƒÙ†Ùƒ Ù‚Ø±Ø§Ø¡ØªÙ‡ Ø¨Ø§Ù„Ø£Ù…Ø±: cat {log_file_path}")
        
        summary_parts = []
        if stats["created"] > 0:
            summary_parts.append(f"âœ… {stats['created']} Ø¬Ø¯ÙŠØ¯")
        if stats["updated"] > 0:
            summary_parts.append(f"ğŸ”„ {stats['updated']} Ù…Ø­Ø¯Ø«")
        if stats["errors"] > 0:
            summary_parts.append(f"âŒ {stats['errors']} Ø®Ø·Ø£")

        upload_log.complete(
            summary=" | ".join(summary_parts) if summary_parts else "Ù…ÙƒØªÙ…Ù„"
        )
        
        # Ø­ÙØ¸ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ù„ÙˆØ¬
        upload_log.options['log_file'] = log_file_path
        upload_log.save(update_fields=['options'])
        
        # Ø¥ØºÙ„Ø§Ù‚ Ù…Ù„Ù Ø§Ù„Ù„ÙˆØ¬
        try:
            log_file.close()
        except Exception:
            pass

        # ğŸš€ ØªØ´ØºÙŠÙ„ Ø®Ø· Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ: ØªØ±Ø­ÙŠÙ„ + QR + Ù…Ø²Ø§Ù…Ù†Ø© Cloudflare
        if affected_product_ids:
            try:
                from inventory.auto_product_pipeline import bulk_post_upload_pipeline
                log_message(f"ğŸ”„ Ø¨Ø¯Ø¡ Ø®Ø· Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù€ {len(affected_product_ids)} Ù…Ù†ØªØ¬...")
                bulk_post_upload_pipeline(affected_product_ids)
                log_message(f"âœ… ØªÙ… ØªØ´ØºÙŠÙ„ Ø®Ø· Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ (ØªØ±Ø­ÙŠÙ„ + QR + Ù…Ø²Ø§Ù…Ù†Ø© Cloudflare)")
            except Exception as pipeline_error:
                log_message(f"âš ï¸ ØªØ­Ø°ÙŠØ±: ÙØ´Ù„ ØªØ´ØºÙŠÙ„ Ø®Ø· Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ: {pipeline_error}")
                logger.error(f"Auto pipeline error after bulk upload: {pipeline_error}")

        return {"status": "success", "stats": stats}

    except Exception as e:
        log_message(f"\nğŸš¨ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}")
        import traceback
        log_message(f"ğŸ“ Traceback:\n{traceback.format_exc()}")
        
        try:
            log_file.close()
        except Exception:
            pass
            
        if "upload_log" in locals():
            upload_log.fail(error_message=str(e))
        raise

    finally:
        # Ø¥Ø¹Ø§Ø¯Ø© ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù€ signals
        post_save.receivers = original_post_save
        pre_save.receivers = original_pre_save
        log_message("âš¡ ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªÙØ¹ÙŠÙ„ Signals")
