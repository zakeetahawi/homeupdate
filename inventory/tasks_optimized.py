"""
Ù…Ù‡Ø§Ù… Celery Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù„Ù…Ø®Ø²ÙˆÙ† - Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ ÙŠÙ…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
"""

from celery import shared_task
from django.utils import timezone
from django.db import transaction
from decimal import Decimal
import logging
import pandas as pd
from io import BytesIO

logger = logging.getLogger(__name__)


@shared_task(bind=True, time_limit=600, soft_time_limit=540, rate_limit=None)
def bulk_upload_products_fast(self, upload_log_id, file_content, warehouse_id, upload_mode, user_id, auto_delete_empty=False):
    """
    Ø±ÙØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© - Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ Ù…Ø­Ø³Ù‘Ù†
    Ø§Ù„Ø£ÙˆØ¶Ø§Ø¹:
    - smart_update: ØªØ­Ø¯ÙŠØ« Ø°ÙƒÙŠ Ù…Ø¹ Ù†Ù‚Ù„ Ù„Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„ØµØ­ÙŠØ­
    - merge_warehouses: Ø¯Ù…Ø¬ Ø§Ù„Ø£ØµÙ†Ø§Ù Ø§Ù„Ù…ÙƒØ±Ø±Ø©  
    - add_only: Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯ ÙÙ‚Ø·
    - clean_start: Ù…Ø³Ø­ ÙƒØ§Ù…Ù„ ÙˆØ¨Ø¯Ø¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯
    
    Args:
        auto_delete_empty: Ø­Ø°Ù Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ© Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡
    """
    from django.contrib.auth import get_user_model
    from .models import (BulkUploadLog, Product, Category, Warehouse, 
                         StockTransaction, BulkUploadError)
    from .smart_upload_logic import (smart_update_product, clean_start_reset,
                                     add_stock_transaction, delete_empty_warehouses)
    
    User = get_user_model()
    logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø±ÙØ¹ Ø§Ù„Ø°ÙƒÙŠ - Log: {upload_log_id} - Ø§Ù„ÙˆØ¶Ø¹: {upload_mode}")
    
    try:
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ø¨Ø¯ÙˆÙ† select_for_update)
        upload_log = BulkUploadLog.objects.get(id=upload_log_id)
        user = User.objects.get(id=user_id)
        warehouse = Warehouse.objects.get(id=warehouse_id) if warehouse_id else None
        
        upload_log.status = 'processing'
        upload_log.save(update_fields=['status'])
        
        # Ù‚Ø±Ø§Ø¡Ø© Excel Ø¨Ø³Ø±Ø¹Ø©
        logger.info("ğŸ“Š Ù‚Ø±Ø§Ø¡Ø© Excel...")
        df = pd.read_excel(BytesIO(file_content), engine='openpyxl')
        total = len(df)
        
        # ØªØ­Ø¯ÙŠØ« total_rows Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© - Ù…Ù‡Ù… Ù„Ù„Ù€ API!
        upload_log.total_rows = total
        upload_log.save(update_fields=['total_rows'])
        
        logger.info(f"ğŸ“‹ {total} ØµÙ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
        
        # ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ù…Ù„Ø© Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨
        if upload_mode == 'clean_start':
            logger.warning("âš ï¸ ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ù…Ù„")
            reset_stats = clean_start_reset()
            upload_log.summary = f"Ù…Ø³Ø­ ÙƒØ§Ù…Ù„: {reset_stats['deleted_products']} Ù…Ù†ØªØ¬ØŒ {reset_stats['deleted_transactions']} Ù…Ø¹Ø§Ù…Ù„Ø©"
            upload_log.save(update_fields=['summary'])
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - ÙÙ‚Ø· Ø§Ù„Ø§Ø³Ù… Ù…Ø·Ù„ÙˆØ¨ (Ø§Ù„Ø³Ø¹Ø± Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©)
        df = df.dropna(subset=['Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬']).fillna('')
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©
        categories_cache = {c.name: c for c in Category.objects.all()}
        warehouses_cache = {w.name: w for w in Warehouse.objects.filter(is_active=True)}
        
        stats = {
            'created': 0, 
            'updated': 0, 
            'moved': 0, 
            'merged': 0, 
            'skipped': 0, 
            'errors': 0,
            'cutting_updated': 0,
            'cutting_split': 0
        }
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø¯ÙØ¹Ø§Øª Ø³Ø±ÙŠØ¹Ø© Ø¬Ø¯Ø§Ù‹ - 10x Ø£Ø³Ø±Ø¹!
        batch_size = 1000  # Ø²ÙŠØ§Ø¯Ø© Ù…Ù† 100 Ø¥Ù„Ù‰ 1000
        errors_batch = []
        last_progress_update = 0  # Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù… ÙƒÙ„ 5%
        
        logger.info(f"ğŸ”„ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(df)} ØµÙ ÙÙŠ Ø¯ÙØ¹Ø§Øª Ø¨Ø­Ø¬Ù… {batch_size}")
        
        for batch_start in range(0, len(df), batch_size):
            batch_end = min(batch_start + batch_size, len(df))
            batch = df.iloc[batch_start:batch_end]
            
            logger.info(f"ğŸ“¦ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø© {batch_start}-{batch_end} ({len(batch)} ØµÙ)")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙÙˆÙ Ø¨Ø³Ø±Ø¹Ø© - Ø¨Ø¯ÙˆÙ† atomic Ù„ÙƒÙ„ ØµÙ
            for idx, row in batch.iterrows():
                try:
                    logger.info(f"ğŸ” Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙ {idx + 2}")
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
                    name = str(row['Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬']).strip()
                    code = str(row.get('Ø§Ù„ÙƒÙˆØ¯', '')).strip() or None
                    
                    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙˆØ¯: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£ØµÙØ§Ø± Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø© Ù„Ø£Ù† Excel ÙŠØ­Ø°ÙÙ‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
                    # Ù…Ø«Ø§Ù„: 010100100730 ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… â†’ 10100100730 ÙÙŠ Excel
                    if code and code.isdigit():
                        code = code.lstrip('0') or '0'  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£ØµÙØ§Ø± Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø©ØŒ Ù„ÙƒÙ† Ø§Ø­ØªÙØ¸ Ø¨Ù€ '0' Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙƒÙˆØ¯ ÙƒÙ„Ù‡ Ø£ØµÙØ§Ø±
                    
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¹Ø± Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†
                    try:
                        price_value = row.get('Ø§Ù„Ø³Ø¹Ø±', '')
                        if pd.notna(price_value) and str(price_value).strip() not in ['', 'nan', 'none']:
                            price = float(price_value)
                        else:
                            price = 0
                    except (ValueError, TypeError):
                        price = 0
                    
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒÙ…ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†
                    try:
                        quantity_value = row.get('Ø§Ù„ÙƒÙ…ÙŠØ©', '')
                        if pd.notna(quantity_value) and str(quantity_value).strip() not in ['', 'nan', 'none']:
                            quantity = float(quantity_value)
                        else:
                            quantity = 0
                    except (ValueError, TypeError):
                        quantity = 0
                    
                    # Ø§Ù„ÙˆØµÙ
                    description = str(row.get('Ø§Ù„ÙˆØµÙ', '')).strip() if pd.notna(row.get('Ø§Ù„ÙˆØµÙ')) else ''
                    
                    # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ù…Ø®Ø²ÙˆÙ†
                    try:
                        min_stock_value = str(row.get('Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰', 0)).strip() if pd.notna(row.get('Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰')) else '0'
                        minimum_stock = int(float(min_stock_value)) if min_stock_value and min_stock_value.lower() not in ['', 'nan', 'none'] else 0
                    except (ValueError, TypeError):
                        minimum_stock = 0
                    
                    # Ø§Ù„Ø¹Ù…Ù„Ø© ÙˆØ§Ù„ÙˆØ­Ø¯Ø©
                    currency = str(row.get('Ø§Ù„Ø¹Ù…Ù„Ø©', 'EGP')).strip().upper()
                    if currency not in ['EGP', 'USD', 'EUR']:
                        currency = 'EGP'
                    unit = str(row.get('Ø§Ù„ÙˆØ­Ø¯Ø©', 'piece')).strip() or 'piece'
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
                    # Ù„Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©: ÙŠÙ…ÙƒÙ† ØªØ±Ùƒ Ø§Ù„Ø§Ø³Ù… ÙØ§Ø±ØºØ§Ù‹ (Ù„ØªØ­Ø¯ÙŠØ« Ø­Ù‚ÙˆÙ„ Ø£Ø®Ø±Ù‰ ÙÙ‚Ø·)
                    # Ù„Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©: Ø§Ù„Ø§Ø³Ù… Ù…Ø·Ù„ÙˆØ¨
                    is_existing_product = False
                    actual_code = code  # Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙØ¹Ù„ÙŠ Ø§Ù„Ø°ÙŠ Ø³ÙŠÙØ³ØªØ®Ø¯Ù…
                    
                    if code:
                        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„ÙƒÙˆØ¯ ÙƒÙ…Ø§ Ù‡Ùˆ
                        is_existing_product = Product.objects.filter(code=code).exists()
                        
                        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙØ¹Ø«Ø± Ø¹Ù„ÙŠÙ‡ØŒ Ø¬Ø±Ø¨ Ù…Ø¹ Ø§Ù„Ø£ØµÙØ§Ø± Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø©
                        if not is_existing_product and code.isdigit():
                            # ØªØ¬Ø±Ø¨Ø© Ø£Ø·ÙˆØ§Ù„ Ù…Ø®ØªÙ„ÙØ© (Ù…Ù† Ø·ÙˆÙ„ Ø§Ù„ÙƒÙˆØ¯+1 Ø­ØªÙ‰ 15)
                            max_length = max(len(code) + 5, 15)
                            for padding in range(len(code) + 1, max_length + 1):
                                padded_code = code.zfill(padding)
                                if Product.objects.filter(code=padded_code).exists():
                                    is_existing_product = True
                                    actual_code = padded_code  # Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø¨Ø·Ù†
                                    logger.info(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ØªØ¬: {code} -> {padded_code}")
                                    break
                    
                    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ù†ØªØ¬ Ø¬Ø¯ÙŠØ¯ Ø¨Ø¯ÙˆÙ† Ø§Ø³Ù…ØŒ Ø±ÙØ¶
                    if not is_existing_product and not name:
                        stats['errors'] += 1
                        errors_batch.append(BulkUploadError(
                            upload_log=upload_log,
                            row_number=idx + 2,
                            error_type='missing_data',
                            result_status='failed',
                            error_message='Ù…Ù†ØªØ¬ Ø¬Ø¯ÙŠØ¯ ÙŠØªØ·Ù„Ø¨ Ø§Ø³Ù…',
                            row_data=row.to_dict()
                        ))
                        continue
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø³Ø¹Ø± - ÙÙ‚Ø· Ù„Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
                    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ù†ØªØ¬ Ø¬Ø¯ÙŠØ¯ Ø¨Ø¯ÙˆÙ† Ø³Ø¹Ø±ØŒ Ø±ÙØ¶
                    if not is_existing_product and price <= 0:
                        stats['errors'] += 1
                        errors_batch.append(BulkUploadError(
                            upload_log=upload_log,
                            row_number=idx + 2,
                            error_type='missing_data',
                            result_status='failed',
                            error_message='Ù…Ù†ØªØ¬ Ø¬Ø¯ÙŠØ¯ ÙŠØªØ·Ù„Ø¨ Ø³Ø¹Ø± ØµØ­ÙŠØ­ (> 0)',
                            row_data=row.to_dict()
                        ))
                        continue
                    
                    # Ø§Ù„ÙØ¦Ø©
                    cat_name = str(row.get('Ø§Ù„ÙØ¦Ø©', '')).strip()
                    category = None
                    if cat_name:
                        if cat_name in categories_cache:
                            category = categories_cache[cat_name]
                        else:
                            category = Category.objects.create(name=cat_name)
                            categories_cache[cat_name] = category
                    
                    # Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù
                    wh_name = str(row.get('Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹', '')).strip()
                    target_wh = warehouse  # Ù…Ù† ØµÙØ­Ø© Ø§Ù„Ø±ÙØ¹
                    
                    # Ø¥Ø°Ø§ ØªÙ… ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆØ¯Ø¹ ÙÙŠ Ø§Ù„Ù…Ù„ÙØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡
                    if wh_name:
                        if wh_name in warehouses_cache:
                            target_wh = warehouses_cache[wh_name]
                        else:
                            # Ø¥Ù†Ø´Ø§Ø¡/Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
                            from .views_bulk import get_or_create_warehouse
                            target_wh = get_or_create_warehouse(wh_name, user)
                            if target_wh:
                                warehouses_cache[wh_name] = target_wh
                            else:
                                raise ValueError(f'ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡/Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹: {wh_name}')
                    
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    product_data = {
                        'name': name,
                        'code': actual_code,  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙØ¹Ù„ÙŠ (Ø§Ù„Ù…Ø¨Ø·Ù† Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±)
                        'price': price,
                        'category': category,
                        'quantity': quantity,
                        'description': description,
                        'minimum_stock': minimum_stock,
                        'currency': currency,
                        'unit': unit
                    }
                    
                    result = smart_update_product(product_data, target_wh, user, upload_mode)
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                    if result['action'] == 'created':
                        stats['created'] += 1
                    elif result['action'] == 'updated':
                        stats['updated'] += 1
                    elif result['action'] == 'moved':
                        stats['moved'] += 1
                        stats['updated'] += 1
                    elif result['action'] == 'skipped':
                        stats['skipped'] += 1
                        errors_batch.append(BulkUploadError(
                            upload_log=upload_log,
                            row_number=idx + 2,
                            error_type='duplicate',
                            result_status='skipped',
                            error_message=result['message'],
                            row_data=row.to_dict()
                        ))
                    
                    # ØªØªØ¨Ø¹ ØªØ­Ø¯ÙŠØ«Ø§Øª Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØªÙ‚Ø·ÙŠØ¹
                    if 'cutting_orders_updated' in result:
                        stats['cutting_updated'] += result['cutting_orders_updated']
                    if 'cutting_orders_split' in result:
                        stats['cutting_split'] += result['cutting_orders_split']
                
                except Exception as e:
                    logger.error(f"Ø®Ø·Ø£ ØµÙ {idx + 2}: {e}")
                    stats['errors'] += 1
                    errors_batch.append(BulkUploadError(
                        upload_log=upload_log,
                        row_number=idx + 2,
                        error_type='processing',
                        result_status='failed',
                        error_message=str(e),
                        row_data=row.to_dict() if hasattr(row, 'to_dict') else {}
                    ))
            
            # Ø­ÙØ¸ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø¨Ø§Ù„Ø¯ÙØ¹Ø© (Ø£Ø³Ø±Ø¹)
            if errors_batch:
                BulkUploadError.objects.bulk_create(errors_batch, batch_size=500)
                errors_batch = []
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù… - ÙÙ‚Ø· ÙƒÙ„ 5% Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª
            processed = batch_end
            percent = int((processed / total) * 100)
            
            if percent >= last_progress_update + 5 or processed == total:
                upload_log.processed_count = processed
                upload_log.created_count = stats['created']
                upload_log.updated_count = stats['updated']
                upload_log.skipped_count = stats['skipped']
                upload_log.error_count = stats['errors']
                upload_log.save(update_fields=[
                    'processed_count', 'created_count', 'updated_count',
                    'skipped_count', 'error_count'
                ])
                
                # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ù…Ø©
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'current': processed,
                        'total': total,
                        'percent': percent,
                        'created': stats['created'],
                        'updated': stats['updated'],
                        'skipped': stats['skipped'],
                        'errors': stats['errors'],
                        'speed': int(processed / max(1, (timezone.now().timestamp() - upload_log.created_at.timestamp())))
                    }
                )
                
                logger.info(f"âš¡ {percent}% - {processed}/{total}")
                last_progress_update = percent
        
        # Ø¥ÙƒÙ…Ø§Ù„
        summary_parts = []
        if stats['created'] > 0:
            summary_parts.append(f"âœ… {stats['created']} Ù…Ù†ØªØ¬ Ø¬Ø¯ÙŠØ¯")
        if stats['updated'] > 0:
            summary_parts.append(f"ğŸ”„ {stats['updated']} Ù…Ø­Ø¯Ø«")
        if stats['moved'] > 0:
            summary_parts.append(f"ğŸ“¦ {stats['moved']} Ù†ÙÙ‚Ù„ Ù„Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„ØµØ­ÙŠØ­")
        if stats['cutting_updated'] > 0:
            summary_parts.append(f"ğŸ”ª {stats['cutting_updated']} Ø£Ù…Ø± ØªÙ‚Ø·ÙŠØ¹ Ù…Ø­Ø¯Ø«")
        if stats['cutting_split'] > 0:
            summary_parts.append(f"ğŸ”€ {stats['cutting_split']} Ø£Ù…Ø± ØªÙ‚Ø·ÙŠØ¹ Ù…Ù†Ù‚Ø³Ù…")
        if stats['skipped'] > 0:
            summary_parts.append(f"â­ï¸ {stats['skipped']} Ù…ØªØ®Ø·Ù‰")
        if stats['errors'] > 0:
            summary_parts.append(f"âŒ {stats['errors']} Ø®Ø·Ø£")
        
        summary = " | ".join(summary_parts) if summary_parts else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª"
        
        upload_log.complete(summary=summary)
        
        logger.info("ğŸ‰ Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø±ÙØ¹ Ø¨Ù†Ø¬Ø§Ø­!")
        
        # Ø­Ø°Ù Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ© Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ø°Ù„Ùƒ
        deleted_warehouses = []
        if auto_delete_empty:
            logger.info("ğŸ—‘ï¸ Ø¨Ø¯Ø¡ Ø­Ø°Ù Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ©...")
            delete_result = delete_empty_warehouses(user)
            deleted_warehouses = delete_result.get('warehouses', [])
            
            if deleted_warehouses:
                logger.info(f"âœ… ØªÙ… Ø­Ø°Ù {len(deleted_warehouses)} Ù…Ø³ØªÙˆØ¯Ø¹: {', '.join(deleted_warehouses)}")
                upload_log.summary = summary + f" | ğŸ—‘ï¸ Ø­ÙØ°Ù {len(deleted_warehouses)} Ù…Ø³ØªÙˆØ¯Ø¹ ÙØ§Ø±Øº"
                upload_log.save(update_fields=['summary'])
        
        return {
            'status': 'success',
            'stats': stats,
            'upload_log_id': upload_log_id,
            'deleted_warehouses': deleted_warehouses
        }
    
    except Exception as e:
        logger.error(f"ğŸ’¥ Ø®Ø·Ø£ ÙƒØ§Ø±Ø«ÙŠ: {e}")
        upload_log.fail(error_message=str(e))
        raise
