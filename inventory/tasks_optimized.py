"""
Ù…Ù‡Ø§Ù… Celery Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù„Ù…Ø®Ø²ÙˆÙ† - Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ ÙŠÙ…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
"""

import logging
from decimal import Decimal
from io import BytesIO

import pandas as pd
from celery import shared_task
from django.db import transaction
from django.utils import timezone

logger = logging.getLogger(__name__)


@shared_task(bind=True, time_limit=600, soft_time_limit=540, rate_limit=None)
def bulk_upload_products_fast(
    self,
    upload_log_id,
    file_content,
    warehouse_id,
    upload_mode,
    user_id,
    auto_delete_empty=False,
):
    """
    Ø±ÙØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© - Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ Ù…Ø­Ø³Ù‘Ù†
    """
    from django.contrib.auth import get_user_model

    from .models import (
        BaseProduct,
        BulkUploadError,
        BulkUploadLog,
        Category,
        Product,
        ProductVariant,
        StockTransaction,
        Warehouse,
    )
    from .smart_upload_logic import (
        add_stock_transaction,
        clean_start_reset,
        delete_empty_warehouses,
        smart_update_product,
    )

    User = get_user_model()
    logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø±ÙØ¹ Ø§Ù„Ø°ÙƒÙŠ - Log: {upload_log_id} - Ø§Ù„ÙˆØ¶Ø¹: {upload_mode}")

    # ØªØ¹Ø·ÙŠÙ„ Cloudflare signals Ù„Ù…Ù†Ø¹ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    from django.db.models.signals import post_save, pre_save

    from .models import BaseProduct, ProductVariant

    # Ø­ÙØ¸ receivers Ø§Ù„Ø£ØµÙ„ÙŠØ©
    original_post_save = list(post_save.receivers)
    original_pre_save = list(pre_save.receivers)

    # ØªØ¹Ø·ÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù€ signals
    post_save.receivers = []
    pre_save.receivers = []
    logger.info("âš¡ ØªÙ… ØªØ¹Ø·ÙŠÙ„ Signals Ù„Ù„Ø³Ø±Ø¹Ø©")

    try:
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        upload_log = BulkUploadLog.objects.get(id=upload_log_id)
        user = User.objects.get(id=user_id)
        warehouse = Warehouse.objects.get(id=warehouse_id) if warehouse_id else None

        upload_log.status = "processing"
        upload_log.save(update_fields=["status"])

        # Ù‚Ø±Ø§Ø¡Ø© Excel Ø¨Ø³Ø±Ø¹Ø©
        logger.info("ğŸ“Š Ù‚Ø±Ø§Ø¡Ø© Excel...")
        df = pd.read_excel(BytesIO(file_content), engine="openpyxl")
        total = len(df)

        # ØªØ­Ø¯ÙŠØ« total_rows Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
        upload_log.total_rows = total
        upload_log.save(update_fields=["total_rows"])

        logger.info(f"ğŸ“‹ {total} ØµÙ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")

        # ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ù…Ù„Ø© Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨
        if upload_mode == "clean_start":
            logger.warning("âš ï¸ ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ù…Ù„")
            reset_stats = clean_start_reset()
            upload_log.summary = f"Ù…Ø³Ø­ ÙƒØ§Ù…Ù„: {reset_stats['deleted_products']} Ù…Ù†ØªØ¬ØŒ {reset_stats['deleted_transactions']} Ù…Ø¹Ø§Ù…Ù„Ø©"
            upload_log.save(update_fields=["summary"])

        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„Ù…Ø³Ø­
        subset_cols = []
        name_cols = ["Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬", "Ø§Ù„Ø§Ø³Ù…", "product_name", "name"]
        code_cols = ["Ø§Ù„ÙƒÙˆØ¯", "ÙƒÙˆØ¯ Ø§Ù„Ù…Ù†ØªØ¬", "product_code", "code"]

        for col in name_cols + code_cols:
            if col in df.columns:
                subset_cols.append(col)

        if subset_cols:
            df = df.dropna(how="all", subset=subset_cols).fillna("")
        else:
            df = df.dropna(how="all").fillna("")

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¨Ø­Ø« (Caching)
        # Ù‡Ø°Ø§ ÙŠÙ‚Ù„Ù„ Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù† O(N) Ø¥Ù„Ù‰ O(1)
        logger.info("ğŸ§  Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        categories_cache = {c.name.strip(): c for c in Category.objects.all()}
        warehouses_cache = {
            w.name.strip(): w for w in Warehouse.objects.filter(is_active=True)
        }

        # ÙƒØ§Ø´ Ù„Ù„Ù…Ù†ØªØ¬Ø§Øª (Legacy Products) - Ù…Ø¹ ÙƒÙ„ Ù†Ø³Ø® Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        products_cache = {}
        for p in Product.objects.all():
            if p.code:
                products_cache[p.code] = p
                # Ø¥Ø¶Ø§ÙØ© Ù†Ø³Ø® Ø¨Ø£ØµÙØ§Ø± Ø¨Ø§Ø¯Ø¦Ø© Ù…Ø®ØªÙ„ÙØ© Ù„ØªØ¬Ù†Ø¨ IntegrityError
                if p.code.isdigit():
                    for padding in range(len(p.code), 15):
                        padded = p.code.zfill(padding)
                        if padded not in products_cache:
                            products_cache[padded] = p

        # ÙƒØ§Ø´ Ù„Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Base Products)
        base_products_cache = {
            bp.code: bp for bp in BaseProduct.objects.all() if bp.code
        }

        # ÙƒØ§Ø´ Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª (Variants) Ù…ÙÙ‡Ø±Ø³Ø© Ø¨Ù€ legacy_product_id
        variants_cache = {
            v.legacy_product_id: v
            for v in ProductVariant.objects.filter(
                legacy_product_id__isnull=False
            ).select_related("base_product")
        }

        data_cache = {
            "products": products_cache,
            "base_products": base_products_cache,
            "variants": variants_cache,
        }

        stats = {
            "created": 0,
            "updated": 0,
            "moved": 0,
            "merged": 0,
            "skipped": 0,
            "errors": 0,
            "cutting_updated": 0,
            "cutting_split": 0,
        }

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø¯ÙØ¹Ø§Øª Ù…ØªÙˆØ³Ø·Ø© Ù„Ù„ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø£Ù…Ø§Ù†
        batch_size = 50  # Ø¯ÙØ¹Ø§Øª Ø£ØµØºØ± Ù„ØªÙ‚Ù„ÙŠÙ„ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        results_batch = []
        processed_overall = 0

        logger.info(f"ğŸ”„ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(df)} ØµÙ Ø¨Ø³Ø¹Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª ÙƒØ§Ù…Ù„Ø©")

        for batch_start in range(0, len(df), batch_size):
            batch_end = min(batch_start + batch_size, len(df))
            batch = df.iloc[batch_start:batch_end]

            logger.info(f"ğŸ“¦ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø© {batch_start}-{batch_end}")

            from django.db import transaction

            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙÙˆÙ - Ù…Ø¹Ø§Ù…Ù„Ø© Ù„ÙƒÙ„ 10 ØµÙÙˆÙ Ù„Ù„ØªÙˆØ§Ø²Ù†
            mini_batch_size = 10
            for mini_start in range(0, len(batch), mini_batch_size):
                mini_end = min(mini_start + mini_batch_size, len(batch))
                mini_batch = batch.iloc[mini_start:mini_end]
                
                try:
                    with transaction.atomic():
                        for i, (idx, row) in enumerate(mini_batch.iterrows()):
                            processed_overall += 1
                            
                            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø±Ù‚Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ (Ù…Ù† Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ù…ÙÙˆÙ„Ù‘Ø¯)
                            # 0=Ø§Ø³Ù…ØŒ 1=ÙƒÙˆØ¯ØŒ 2=ÙØ¦Ø©ØŒ 3=Ø³Ø¹Ø±ØŒ 4=Ø¬Ù…Ù„Ø©ØŒ 5=ÙƒÙ…ÙŠØ©ØŒ 6=Ù…Ø³ØªÙˆØ¯Ø¹ØŒ 7=ÙˆØµÙ

                            def safe_get(index, default=None):
                                """Ù‚Ø±Ø§Ø¡Ø© Ù‚ÙŠÙ…Ø© Ù…Ù† Ø¹Ù…ÙˆØ¯ Ø¨Ø±Ù‚Ù…Ù‡"""
                                if index < len(row):
                                    val = row.iloc[index]
                                    if pd.notna(val):
                                        val = str(val).strip()
                                        if val and val.lower() not in ["nan", "none", ""]:
                                            return val
                                return default

                            # Ø§Ù„ÙƒÙˆØ¯ (Ø¹Ù…ÙˆØ¯ 1) - Ø§Ù„Ø£Ù‡Ù…
                            code = safe_get(1)
                            if code and code.isdigit():
                                code = code.lstrip("0") or "0"

                            # Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬ (Ø¹Ù…ÙˆØ¯ 0)
                            name = safe_get(0, "")

                            # Ø§Ù„Ø³Ø¹Ø± (Ø¹Ù…ÙˆØ¯ 3)
                            try:
                                price = float(safe_get(3, "0"))
                            except:
                                price = 0

                            # Ø³Ø¹Ø± Ø§Ù„Ø¬Ù…Ù„Ø© - ØªØ¬Ø±Ø¨Ø© Ø¹Ø¯Ø© Ø£Ø¹Ù…Ø¯Ø© (4 Ø«Ù… 1 Ù„Ù„Ù…Ù„ÙØ§Øª Ø¨Ø¹Ù…ÙˆØ¯ÙŠÙ†)
                            wholesale_price = None
                            for ws_col in [4, 1]:  # Ø§Ù„Ù‚Ø§Ù„Ø¨ = 4ØŒ Ù…Ù„Ù Ø¨Ø¹Ù…ÙˆØ¯ÙŠÙ† = 1
                                try:
                                    ws_val = safe_get(ws_col)
                                    if ws_val:
                                        wholesale_price = float(ws_val)
                                        break
                                except:
                                    continue

                            # Ø§Ù„ÙƒÙ…ÙŠØ© (Ø¹Ù…ÙˆØ¯ 5)
                            try:
                                quantity = float(safe_get(5, "0"))
                            except:
                                quantity = 0

                            # Ø§Ù„ÙˆØµÙ (Ø¹Ù…ÙˆØ¯ 7)
                            description = safe_get(7, "")

                            # DEBUG: Ø·Ø¨Ø§Ø¹Ø© Ø£ÙˆÙ„ 3 ØµÙÙˆÙ
                            if processed_overall <= 3:
                                logger.warning(f"ğŸ” DEBUG ØµÙ {processed_overall}:")
                                logger.warning(f"   Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù„Ù: {list(row.index)}")
                                logger.warning(
                                    f"   ÙƒÙˆØ¯={code}, Ø³Ø¹Ø±={price}, Ø¬Ù…Ù„Ø©={wholesale_price}"
                                )

                            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ù†ØªØ¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒØ§Ø´
                            is_existing = False
                            actual_code = code
                            if code and "products" in data_cache:
                                if code in data_cache["products"]:
                                    is_existing = True
                                    actual_code = code
                                elif code.isdigit():
                                    for p in range(len(code), 15):
                                        padded = code.zfill(p)
                                        if padded in data_cache["products"]:
                                            is_existing = True
                                            actual_code = padded
                                            break

                            if not is_existing and not name:
                                stats["errors"] += 1
                                results_batch.append(
                                    BulkUploadError(
                                        upload_log=upload_log,
                                        row_number=idx + 2,
                                        error_type="missing_data",
                                        result_status="failed",
                                        error_message="Ù…Ù†ØªØ¬ Ø¬Ø¯ÙŠØ¯ ÙˆÙ„ÙƒÙ† Ø§Ù„Ø§Ø³Ù… Ù…ÙÙ‚ÙˆØ¯ - ÙŠØ±Ø¬Ù‰ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø§Ø³Ù… Ù„ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù† Ø¥Ù†Ø´Ø§Ø¦Ù‡",
                                        row_data={"code": actual_code},
                                    )
                                )
                                continue

                            # ØªØ¬Ù…ÙŠØ¹
                            product_data = {
                                "name": name,
                                "code": actual_code,
                                "price": price,
                                "wholesale_price": wholesale_price,
                                "quantity": quantity,
                            }

                            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙØ¦Ø© ÙˆØ§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ø§
                            cat_name = str(row.get("Ø§Ù„ÙØ¦Ø©", "")).strip()
                            if cat_name:
                                if cat_name in categories_cache:
                                    product_data["category"] = categories_cache[cat_name]
                                else:
                                    cat = Category.objects.create(name=cat_name)
                                    categories_cache[cat_name] = cat
                                    product_data["category"] = cat

                            wh_name = str(row.get("Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹", "")).strip()
                            target_wh = warehouse
                            if wh_name:
                                if wh_name in warehouses_cache:
                                    target_wh = warehouses_cache[wh_name]
                                else:
                                    from .views_bulk import get_or_create_warehouse

                                    target_wh = get_or_create_warehouse(wh_name, user)
                                    if target_wh:
                                        warehouses_cache[wh_name] = target_wh

                            # Ø­ÙØ¸
                            result = smart_update_product(
                                product_data,
                                target_wh,
                                user,
                                upload_mode,
                                cache=data_cache,
                                fast_mode=True,
                            )

                            action = result["action"]
                            if action == "created":
                                stats["created"] += 1
                            elif action == "updated":
                                stats["updated"] += 1
                            elif action == "moved":
                                stats["moved"] += 1
                                stats["updated"] += 1
                            elif action == "skipped":
                                stats["skipped"] += 1

                            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„ØªÙ‚Ø±ÙŠØ± (Ø«Ù…Ù† Ù…Ø¹Ù†ÙˆÙŠ Ø¨Ø³ÙŠØ· Ù„Ù„Ø£Ø¯Ø§Ø¡ Ù…Ù‚Ø§Ø¨Ù„ Ø¯Ù‚Ø© Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±)
                            results_batch.append(
                                BulkUploadError(
                                    upload_log=upload_log,
                                    row_number=idx + 2,
                                    error_type="other",
                                    result_status=(
                                        action if action != "moved" else "updated"
                                    ),
                                    error_message=result.get("message", ""),
                                    row_data={
                                        "name": name or code or "Ø¨Ø¯ÙˆÙ† Ø§Ø³Ù…",
                                        "code": actual_code,
                                    },
                                )
                            )
                            
                except Exception as mini_batch_error:
                    # ÙØ´Ù„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø© Ø§Ù„ØµØºÙŠØ±Ø© - ØªØ³Ø¬ÙŠÙ„ ÙƒÙ„ Ø§Ù„ØµÙÙˆÙ ÙƒØ£Ø®Ø·Ø§Ø¡
                    logger.error(f"âŒ ÙØ´Ù„ mini-batch: {mini_batch_error}")
                    for i, (idx, row) in enumerate(mini_batch.iterrows()):
                        if idx + 2 not in [r.row_number for r in results_batch]:
                            stats["errors"] += 1
                            results_batch.append(
                                BulkUploadError(
                                    upload_log=upload_log,
                                    row_number=idx + 2,
                                    error_type="processing",
                                    result_status="failed",
                                    error_message=str(mini_batch_error)[:500],
                                    row_data={},
                                )
                            )

            # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø¹Ø¯ ÙƒÙ„ Ø¯ÙØ¹Ø© ÙƒØ¨ÙŠØ±Ø©
            if results_batch:
                try:
                    BulkUploadError.objects.bulk_create(results_batch)
                    results_batch = []
                except Exception as save_error:
                    logger.error(f"âš ï¸ ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: {save_error}")

            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù… *Ø®Ø§Ø±Ø¬* Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø© Ù„Ø¶Ù…Ø§Ù† Ø¸Ù‡ÙˆØ±Ù‡ Ù„Ù„ÙˆØ§Ø¬Ù‡Ø© ÙÙˆØ±Ø§Ù‹
            percent = int((processed_overall / total) * 100)
            upload_log.processed_count = processed_overall
            upload_log.created_count = stats["created"]
            upload_log.updated_count = stats["updated"]
            upload_log.skipped_count = stats["skipped"]
            upload_log.error_count = stats["errors"]
            upload_log.save(
                update_fields=[
                    "processed_count",
                    "created_count",
                    "updated_count",
                    "skipped_count",
                    "error_count",
                ]
            )

            self.update_state(
                state="PROGRESS",
                meta={
                    "current": processed_overall,
                    "total": total,
                    "percent": percent,
                    "created": stats["created"],
                    "updated": stats["updated"],
                    "skipped": stats["skipped"],
                    "errors": stats["errors"],
                },
            )

        # Ø¥ÙƒÙ…Ø§Ù„
        summary_parts = []
        if stats["created"] > 0:
            summary_parts.append(f"âœ… {stats['created']} Ø¬Ø¯ÙŠØ¯")
        if stats["updated"] > 0:
            summary_parts.append(f"ğŸ”„ {stats['updated']} Ù…Ø­Ø¯Ø«")
        if stats["errors"] > 0:
            summary_parts.append(f"âŒ {stats['errors']} Ø®Ø·Ø£")

        upload_log.complete(
            summary=" | ".join(summary_parts) if summary_parts else "Ù…ÙƒØªÙ…Ù„"
        )
        return {"status": "success", "stats": stats}

    except Exception as e:
        if "upload_log" in locals():
            upload_log.fail(error_message=str(e))
        raise

    finally:
        # Ø¥Ø¹Ø§Ø¯Ø© ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù€ signals
        post_save.receivers = original_post_save
        pre_save.receivers = original_pre_save
        logger.info("âš¡ ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªÙØ¹ÙŠÙ„ Signals")
