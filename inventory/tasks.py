"""
Ù…Ù‡Ø§Ù… Celery Ù„Ù„Ø®Ù„ÙÙŠØ© - Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø®Ø²ÙˆÙ†
"""

import logging
from decimal import Decimal

from celery import shared_task
from django.contrib.auth import get_user_model
from django.utils import timezone

User = get_user_model()
logger = logging.getLogger(__name__)


@shared_task(bind=True, max_retries=1, time_limit=600)
def process_bulk_upload_async(
    self, upload_log_id, file_data, default_warehouse_id, upload_mode, user_id
):
    """
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±ÙØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø¨Ø§Ù„Ø¬Ù…Ù„Ø© ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© - Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù„Ø³Ø±Ø¹Ø©
    """
    import time
    from io import BytesIO

    import pandas as pd
    from django.db import connection, transaction

    from .cache_utils import invalidate_product_cache
    from .models import (
        BulkUploadError,
        BulkUploadLog,
        Category,
        Product,
        StockTransaction,
        Warehouse,
    )
    from .views_bulk import get_or_create_warehouse

    logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±ÙØ¹ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª - Log ID: {upload_log_id}")

    try:
        upload_log = BulkUploadLog.objects.get(id=upload_log_id)
        user = User.objects.get(id=user_id)
        default_warehouse = (
            Warehouse.objects.get(id=default_warehouse_id)
            if default_warehouse_id
            else None
        )

        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø©
        upload_log.status = "processing"
        upload_log.save(update_fields=["status"])

        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø³Ø· ÙˆØ³Ø±ÙŠØ¹
        logger.info("ğŸ“Š Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Excel...")
        try:
            df = pd.read_excel(BytesIO(file_data), engine="openpyxl")
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {str(e)}")
            df = pd.read_excel(BytesIO(file_data))

        total_rows = len(df)
        upload_log.total_rows = total_rows
        upload_log.save(update_fields=["total_rows"])
        logger.info(f"ğŸ“‹ ØªÙ… ØªØ­Ù„ÙŠÙ„ {total_rows} ØµÙ")

        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
        if upload_mode == "full_reset":
            with transaction.atomic():
                from .models import StockTransfer

                StockTransfer.objects.all().delete()
                StockTransaction.objects.all().delete()
                Product.objects.all().delete()

                upload_log.notes = "ØªÙ‡ÙŠØ¦Ø© ÙƒØ§Ù…Ù„Ø©: ØªÙ… Ø­Ø°Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"
                upload_log.save()

        result = {
            "total_processed": 0,
            "created_count": 0,
            "updated_count": 0,
            "created_warehouses": [],
            "errors": [],
        }

        df = df.dropna(subset=["Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬", "Ø§Ù„Ø³Ø¹Ø±"])
        df = df.fillna("")

        errors_to_create = []
        skipped_count = 0

        with transaction.atomic():
            for index, row in df.iterrows():
                row_number = index + 2

                # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù… ÙƒÙ„ 10 Ù…Ù†ØªØ¬Ø§Øª
                if index % 10 == 0:
                    upload_log.processed_count = result["total_processed"]
                    upload_log.save()
                    self.update_state(
                        state="PROGRESS",
                        meta={
                            "current": result["total_processed"],
                            "total": len(df),
                            "percent": int((result["total_processed"] / len(df)) * 100),
                        },
                    )

                try:
                    name = str(row["Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬"]).strip()
                    code = str(row["Ø§Ù„ÙƒÙˆØ¯"]).strip() if pd.notna(row["Ø§Ù„ÙƒÙˆØ¯"]) else None
                    category_name = str(row["Ø§Ù„ÙØ¦Ø©"]).strip()
                    warehouse_name = (
                        str(row.get("Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹", "")).strip()
                        if pd.notna(row.get("Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹"))
                        else ""
                    )

                    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¹Ø± ÙˆØ§Ù„ÙƒÙ…ÙŠØ©
                    try:
                        price = (
                            float(str(row["Ø§Ù„Ø³Ø¹Ø±"]).strip()) if row["Ø§Ù„Ø³Ø¹Ø±"] else 0.0
                        )
                    except Exception:
                        price = 0.0

                    try:
                        quantity = (
                            float(str(row["Ø§Ù„ÙƒÙ…ÙŠØ©"]).strip())
                            if pd.notna(row["Ø§Ù„ÙƒÙ…ÙŠØ©"])
                            else 0.0
                        )
                    except Exception:
                        quantity = 0.0

                    description = str(row.get("Ø§Ù„ÙˆØµÙ", "")).strip()

                    try:
                        minimum_stock = (
                            int(float(str(row.get("Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰", 0)).strip()))
                            if pd.notna(row.get("Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰"))
                            else 0
                        )
                    except Exception:
                        minimum_stock = 0

                    currency = str(row.get("Ø§Ù„Ø¹Ù…Ù„Ø©", "EGP")).strip().upper()
                    unit = str(row.get("Ø§Ù„ÙˆØ­Ø¯Ø©", "piece")).strip()

                    if currency not in ["EGP", "USD", "EUR"]:
                        currency = "EGP"

                    valid_units = [
                        "piece",
                        "kg",
                        "gram",
                        "liter",
                        "meter",
                        "box",
                        "pack",
                        "dozen",
                        "roll",
                        "sheet",
                    ]
                    if unit not in valid_units:
                        unit_map = {
                            "Ù‚Ø·Ø¹Ø©": "piece",
                            "ÙƒÙŠÙ„ÙˆØ¬Ø±Ø§Ù…": "kg",
                            "Ø¬Ø±Ø§Ù…": "gram",
                            "Ù„ØªØ±": "liter",
                            "Ù…ØªØ±": "meter",
                            "Ø¹Ù„Ø¨Ø©": "box",
                            "Ø¹Ø¨ÙˆØ©": "pack",
                            "Ø¯Ø³ØªØ©": "dozen",
                            "Ù„ÙØ©": "roll",
                            "ÙˆØ±Ù‚Ø©": "sheet",
                        }
                        unit = unit_map.get(unit, "piece")

                    if not name or price <= 0:
                        error_msg = "Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬ ÙˆØ§Ù„Ø³Ø¹Ø± Ù…Ø·Ù„ÙˆØ¨Ø§Ù†"
                        result["errors"].append(f"Ø§Ù„ØµÙ {row_number}: {error_msg}")
                        errors_to_create.append(
                            BulkUploadError(
                                upload_log=upload_log,
                                row_number=row_number,
                                error_type="missing_data",
                                result_status="failed",
                                error_message=error_msg,
                                row_data=row.to_dict(),
                            )
                        )
                        continue

                    # Ø§Ù„ÙØ¦Ø©
                    category = None
                    if category_name:
                        category, created = Category.objects.get_or_create(
                            name=category_name,
                            defaults={"description": "ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"},
                        )

                    # Ø§Ù„Ù…Ù†ØªØ¬
                    product = None
                    created = False
                    product_exists = False

                    if code:
                        try:
                            product = Product.objects.get(code=code)
                            product_exists = True

                            if upload_mode == "new_only":
                                skipped_count += 1
                                errors_to_create.append(
                                    BulkUploadError(
                                        upload_log=upload_log,
                                        row_number=row_number,
                                        error_type="duplicate",
                                        result_status="skipped",
                                        error_message=f"Ù…Ù†ØªØ¬ Ù…ÙˆØ¬ÙˆØ¯ - ØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ",
                                        row_data=row.to_dict(),
                                    )
                                )
                                continue

                            elif upload_mode in ["add_to_existing", "replace_quantity"]:
                                product.name = name
                                product.category = category
                                product.description = description
                                product.price = price
                                product.currency = currency
                                product.unit = unit
                                product.minimum_stock = minimum_stock
                                product.save()
                                result["updated_count"] += 1

                        except Product.DoesNotExist:
                            product = Product.objects.create(
                                name=name,
                                code=code,
                                category=category,
                                description=description,
                                price=price,
                                currency=currency,
                                unit=unit,
                                minimum_stock=minimum_stock,
                            )
                            created = True
                            result["created_count"] += 1
                    else:
                        product = Product.objects.create(
                            name=name,
                            category=category,
                            description=description,
                            price=price,
                            currency=currency,
                            unit=unit,
                            minimum_stock=minimum_stock,
                        )
                        created = True
                        result["created_count"] += 1

                    # Ø§Ù„Ù…Ø®Ø²ÙˆÙ†
                    if quantity > 0 and product:
                        target_warehouse = default_warehouse

                        if warehouse_name:
                            target_warehouse = get_or_create_warehouse(
                                warehouse_name, user
                            )
                            if (
                                target_warehouse
                                and target_warehouse.name
                                not in result["created_warehouses"]
                            ):
                                result["created_warehouses"].append(
                                    target_warehouse.name
                                )

                        if not target_warehouse:
                            continue

                        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„ÙƒÙ…ÙŠØ©
                        if upload_mode == "replace_quantity" and product_exists:
                            last_transaction = (
                                StockTransaction.objects.filter(
                                    product=product, warehouse=target_warehouse
                                )
                                .order_by("-transaction_date")
                                .first()
                            )

                            if (
                                last_transaction
                                and last_transaction.running_balance
                                and last_transaction.running_balance > 0
                            ):
                                current_balance = Decimal(
                                    str(last_transaction.running_balance)
                                )
                                StockTransaction.objects.create(
                                    product=product,
                                    warehouse=target_warehouse,
                                    transaction_type="out",
                                    reason="adjustment",
                                    quantity=current_balance,
                                    reference="ØªØµÙÙŠØ± Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„",
                                    notes=f"ØªØµÙÙŠØ± Ø§Ù„Ø±ØµÙŠØ¯ (ÙƒØ§Ù†: {current_balance})",
                                    created_by=user,
                                    transaction_date=timezone.now(),
                                )

                        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙƒÙ…ÙŠØ©
                        StockTransaction.objects.create(
                            product=product,
                            warehouse=target_warehouse,
                            transaction_type="in",
                            reason="purchase",
                            quantity=quantity,
                            reference="Ø±ÙØ¹ Ù…Ù† Ù…Ù„Ù Ø¥ÙƒØ³Ù„",
                            notes=f"Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹: {target_warehouse.name}",
                            created_by=user,
                            transaction_date=timezone.now(),
                        )

                    result["total_processed"] += 1
                    if product:
                        invalidate_product_cache(product.id)

                except Exception as e:
                    error_msg = str(e)
                    result["errors"].append(f"Ø§Ù„ØµÙ {row_number}: {error_msg}")
                    errors_to_create.append(
                        BulkUploadError(
                            upload_log=upload_log,
                            row_number=row_number,
                            error_type="processing",
                            result_status="failed",
                            error_message=error_msg,
                            row_data=row.to_dict() if hasattr(row, "to_dict") else {},
                        )
                    )

        # Ø­ÙØ¸ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        if errors_to_create:
            BulkUploadError.objects.bulk_create(errors_to_create)

        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„
        actual_errors = len(result["errors"]) - skipped_count
        upload_log.processed_count = result["total_processed"]
        upload_log.created_count = result["created_count"]
        upload_log.updated_count = result["updated_count"]
        upload_log.skipped_count = skipped_count
        upload_log.error_count = actual_errors
        upload_log.created_warehouses = result["created_warehouses"]

        summary_parts = []
        if result["created_count"] > 0:
            summary_parts.append(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {result['created_count']} Ù…Ù†ØªØ¬")
        if result["updated_count"] > 0:
            summary_parts.append(f"ØªÙ… ØªØ­Ø¯ÙŠØ« {result['updated_count']} Ù…Ù†ØªØ¬")
        if skipped_count > 0:
            summary_parts.append(f"ØªÙ… ØªØ®Ø·ÙŠ {skipped_count} Ù…Ù†ØªØ¬")
        if actual_errors > 0:
            summary_parts.append(f"ÙØ´Ù„ {actual_errors} ØµÙ")

        summary = ". ".join(summary_parts) if summary_parts else "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª"
        upload_log.complete(summary=summary)

        return {"status": "success", "upload_log_id": upload_log_id, "result": result}

    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±ÙØ¹: {str(e)}")
        upload_log.fail(error_message=str(e))
        raise self.retry(exc=e, countdown=60)


@shared_task(
    bind=True, max_retries=3, default_retry_delay=180, autoretry_for=(Exception,)
)
def cleanup_old_warehouse_data(self, days=90):
    """
    ØªÙ†Ø¸ÙŠÙ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
    """
    from datetime import timedelta

    from django.db import models
    from django.utils import timezone

    from .models import StockTransaction, Warehouse

    cutoff_date = timezone.now() - timedelta(days=days)

    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª Ø¨Ø¯ÙˆÙ† Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø­Ø¯ÙŠØ«Ø©
    inactive_warehouses = (
        Warehouse.objects.filter(is_active=True)
        .exclude(stock_transactions__transaction_date__gte=cutoff_date)
        .annotate(transaction_count=models.Count("stock_transactions"))
        .filter(transaction_count=0)
    )

    count = inactive_warehouses.count()

    logger.info(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {count} Ù…Ø³ØªÙˆØ¯Ø¹ ØºÙŠØ± Ù†Ø´Ø·")

    return {"status": "success", "inactive_count": count}


@shared_task(
    bind=True, max_retries=2, default_retry_delay=300, autoretry_for=(Exception,)
)
def sync_official_fabric_warehouses(self):
    """
    Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ù„Ù„Ø£Ù‚Ù…Ø´Ø©
    """
    from .models import Category, Warehouse

    # Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ù„Ù„Ø£Ù‚Ù…Ø´Ø©
    official_warehouses = [
        {"name": "Ù…Ø³ØªÙˆØ¯Ø¹ Ø§Ù„Ø£Ù‚Ù…Ø´Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ", "code": "FABRIC_MAIN"},
        {"name": "Ù…Ø³ØªÙˆØ¯Ø¹ Ø£Ù‚Ù…Ø´Ø© Ø§Ù„Ø³ØªØ§Ø¦Ø±", "code": "FABRIC_CURTAIN"},
        {"name": "Ù…Ø³ØªÙˆØ¯Ø¹ Ø£Ù‚Ù…Ø´Ø© Ø§Ù„ØªÙ†Ø¬ÙŠØ¯", "code": "FABRIC_UPHOLSTERY"},
    ]

    created = []
    updated = []

    for wh_data in official_warehouses:
        warehouse, was_created = Warehouse.objects.update_or_create(
            code=wh_data["code"],
            defaults={
                "name": wh_data["name"],
                "is_active": True,
                "is_official_fabric_warehouse": True,
                "notes": "Ù…Ø³ØªÙˆØ¯Ø¹ Ø±Ø³Ù…ÙŠ Ù„Ù„Ø£Ù‚Ù…Ø´Ø©",
            },
        )

        if was_created:
            created.append(warehouse.name)
        else:
            updated.append(warehouse.name)

    return {"status": "success", "created": created, "updated": updated}


@shared_task(
    bind=True,
    max_retries=1,
    queue="default",
    name="inventory.tasks.cleanup_old_bulk_upload_errors",
)
def cleanup_old_bulk_upload_errors(self, keep_days=30, keep_per_log=100):
    """
    ØªÙ†Ø¸ÙŠÙ Ø³Ø¬Ù„Ø§Øª BulkUploadError Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØ§Ù„Ù…ÙƒØªØ¸Ø©.

    - ÙŠØ­Ø°Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØªØ§Ø¨Ø¹Ø© Ù„Ø³Ø¬Ù„Ø§Øª Ø£Ù‚Ø¯Ù… Ù…Ù† `keep_days` ÙŠÙˆÙ…Ø§Ù‹
    - ÙŠØ­ØªÙØ¸ Ø¨Ø£ÙˆÙ„ `keep_per_log` Ø®Ø·Ø£ Ù„ÙƒÙ„ Ø³Ø¬Ù„ Ø±ÙØ¹ (Ù„Ù„ØªØ´Ø®ÙŠØµ)
    - ÙŠÙØ´ØºÙÙ‘Ù„ Ø£Ø³Ø¨ÙˆØ¹ÙŠØ§Ù‹ Ø¹Ø¨Ø± Celery Beat

    Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©:
        keep_days=30  â†’ ÙŠØ­Ø°Ù Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø£Ù‚Ø¯Ù… Ù…Ù† 30 ÙŠÙˆÙ…
        keep_per_log=100 â†’ ÙŠØ­ØªÙØ¸ Ø¨Ù€ 100 Ø®Ø·Ø£ Ù„ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ© Ø±ÙØ¹
    """
    from datetime import timedelta

    from django.utils import timezone

    from .models import BulkUploadError, BulkUploadLog

    cutoff = timezone.now() - timedelta(days=keep_days)
    total_deleted = 0

    # 1. Ø­Ø°Ù Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„
    old_logs = BulkUploadLog.objects.filter(created_at__lt=cutoff)
    if old_logs.exists():
        deleted, _ = BulkUploadError.objects.filter(upload_log__in=old_logs).delete()
        total_deleted += deleted
        logger.info(
            f"âœ… Ø­ÙØ°Ù {deleted} Ø®Ø·Ø£ Ø±ÙØ¹ Ù‚Ø¯ÙŠÙ… (Ø£Ù‚Ø¯Ù… Ù…Ù† {keep_days} ÙŠÙˆÙ…)"
        )

    # 2. Ù„Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø©: Ø§Ø­ØªÙØ¸ Ø¨Ø£ÙˆÙ„ keep_per_log Ø®Ø·Ø£ ÙÙ‚Ø·
    recent_logs = BulkUploadLog.objects.filter(created_at__gte=cutoff)
    for log in recent_logs:
        error_ids = (
            BulkUploadError.objects.filter(upload_log=log)
            .order_by("id")
            .values_list("id", flat=True)
        )
        if error_ids.count() > keep_per_log:
            ids_to_keep = list(error_ids[:keep_per_log])
            deleted, _ = (
                BulkUploadError.objects.filter(upload_log=log)
                .exclude(id__in=ids_to_keep)
                .delete()
            )
            total_deleted += deleted

    if total_deleted:
        logger.info(f"âœ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø³Ø¬Ù„Ø§Øª BulkUploadError Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {total_deleted}")
    else:
        logger.info("â„¹ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø³Ø¬Ù„Ø§Øª BulkUploadError ØªØ­ØªØ§Ø¬ Ø­Ø°ÙØ§Ù‹")

    return {"status": "success", "deleted": total_deleted}
