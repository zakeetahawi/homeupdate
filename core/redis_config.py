"""
ğŸ”´ Redis Cluster Configuration - ØªÙƒÙˆÙŠÙ† Redis Ù…ÙˆØ­Ø¯ ÙˆÙ…Ø­Ø³Ù‘Ù†

Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª:
1. Ø¯Ù…Ø¬ 3 Ù‚ÙˆØ§Ø¹Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Redis Ù…Ù†ÙØµÙ„Ø© ÙÙŠ ÙƒØ§Ø´ ÙˆØ§Ø­Ø¯ Ù…Ø¹ prefixes
2. Connection Pooling Ù…Ø­Ø³Ù‘Ù†
3. Serialization Ø£Ø³Ø±Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… msgpack
4. Health checks ÙˆØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
5. Fallback Ø¢Ù…Ù† Ø¹Ù†Ø¯ ÙØ´Ù„ Redis

Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø© Ù„Ù€ settings.py
"""

# =============================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Redis Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø© - Ø§Ù†Ø³Ø® Ù‡Ø°Ø§ Ø¥Ù„Ù‰ settings.py
# =============================================

REDIS_OPTIMIZED_CONFIG = """
# ===========================================
# ğŸ”´ Redis Cache Configuration - Ù…Ø­Ø³Ù‘Ù†
# ===========================================

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø­Ø¯Ø© Ù…Ø¹ prefixes Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 3 Ù…Ù†ÙØµÙ„Ø©
REDIS_URL = 'redis://localhost:6379/0'

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Connection Pool
REDIS_CONNECTION_POOL_OPTIONS = {
    'max_connections': 100,  # Ø²ÙŠØ§Ø¯Ø© Ù…Ù† 50
    'retry_on_timeout': True,
    'socket_timeout': 5,
    'socket_connect_timeout': 5,
    'socket_keepalive': True,
    'health_check_interval': 30,
}

CACHES = {
    # Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ - Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': REDIS_URL,
        'TIMEOUT': 300,  # 5 Ø¯Ù‚Ø§Ø¦Ù‚
        'KEY_PREFIX': 'crm',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_KWARGS': REDIS_CONNECTION_POOL_OPTIONS,
            'SERIALIZER': 'django_redis.serializers.msgpack.MSGPackSerializer',
            'COMPRESSOR': 'django_redis.compressors.zlib.ZlibCompressor',
            'IGNORE_EXCEPTIONS': True,  # Ù„Ø§ ÙŠÙØ´Ù„ Ø¥Ø°Ø§ ÙƒØ§Ù† Redis ØºÙŠØ± Ù…ØªØ§Ø­
        }
    },
    
    # ÙƒØ§Ø´ Ø§Ù„Ø¬Ù„Ø³Ø§Øª - TTL Ø£Ø·ÙˆÙ„
    'session': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': REDIS_URL,
        'TIMEOUT': 86400,  # 24 Ø³Ø§Ø¹Ø©
        'KEY_PREFIX': 'session',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_KWARGS': {
                'max_connections': 30,
                'retry_on_timeout': True,
            },
            'SERIALIZER': 'django_redis.serializers.json.JSONSerializer',
            'IGNORE_EXCEPTIONS': True,
        }
    },
    
    # ÙƒØ§Ø´ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª - TTL Ù‚ØµÙŠØ±
    'query': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': REDIS_URL,
        'TIMEOUT': 60,  # Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ§Ø­Ø¯Ø©
        'KEY_PREFIX': 'query',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_KWARGS': {
                'max_connections': 50,
                'retry_on_timeout': True,
            },
            'SERIALIZER': 'django_redis.serializers.msgpack.MSGPackSerializer',
            'COMPRESSOR': 'django_redis.compressors.zlib.ZlibCompressor',
            'IGNORE_EXCEPTIONS': True,
        }
    },
    
    # ÙƒØ§Ø´ Ø§Ù„ØµÙØ­Ø§Øª - Ù„Ù„Ù€ views Ø§Ù„Ù…ÙƒØ«ÙØ©
    'page': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': REDIS_URL,
        'TIMEOUT': 300,  # 5 Ø¯Ù‚Ø§Ø¦Ù‚
        'KEY_PREFIX': 'page',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_KWARGS': {
                'max_connections': 30,
                'retry_on_timeout': True,
            },
            'SERIALIZER': 'django_redis.serializers.pickle.PickleSerializer',
            'IGNORE_EXCEPTIONS': True,
        }
    },
    
    # ÙƒØ§Ø´ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª - Ù„Ù„Ù€ Materialized Views
    'stats': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': REDIS_URL,
        'TIMEOUT': 600,  # 10 Ø¯Ù‚Ø§Ø¦Ù‚
        'KEY_PREFIX': 'stats',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_KWARGS': {
                'max_connections': 20,
                'retry_on_timeout': True,
            },
            'SERIALIZER': 'django_redis.serializers.msgpack.MSGPackSerializer',
            'IGNORE_EXCEPTIONS': True,
        }
    },
}

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Redis Ù„Ù„Ø¬Ù„Ø³Ø§Øª
SESSION_ENGINE = 'django.contrib.sessions.backends.cache'
SESSION_CACHE_ALIAS = 'session'
SESSION_COOKIE_AGE = 86400  # 24 Ø³Ø§Ø¹Ø©
SESSION_COOKIE_SECURE = True  # ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ ÙÙ‚Ø·
SESSION_SAVE_EVERY_REQUEST = False  # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Celery Ù…Ø¹ Redis (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø³ØªØ®Ø¯Ù…Ø§Ù‹)
# CELERY_BROKER_URL = 'redis://localhost:6379/1'
# CELERY_RESULT_BACKEND = 'redis://localhost:6379/2'
"""


# =============================================
# Helper Classes Ù„Ù„Ù€ Redis
# =============================================

import functools
import hashlib
import logging
from typing import Any, Callable, Optional

from django.core.cache import caches

logger = logging.getLogger("performance")


class CacheManager:
    """
    Ù…Ø¯ÙŠØ± Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…ÙˆØ­Ø¯ - ÙŠØ³Ù‡Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù…Ø®ØªÙ„Ù

    Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    cache_manager = CacheManager()

    # ÙƒØ§Ø´ Ø¹Ø§Ù…
    cache_manager.set('key', 'value')

    # ÙƒØ§Ø´ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª (Ù‚ØµÙŠØ± Ø§Ù„Ù…Ø¯Ù‰)
    cache_manager.set_query('orders_list', orders_data)

    # ÙƒØ§Ø´ Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª (Ø·ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ù‰)
    cache_manager.set_stats('daily_summary', summary_data)
    """

    def __init__(self):
        self._caches = {
            "default": caches["default"],
            "query": caches.get("query", caches["default"]),
            "page": caches.get("page", caches["default"]),
            "stats": caches.get("stats", caches["default"]),
            "session": caches.get("session", caches["default"]),
        }

    def get_cache(self, cache_type: str = "default"):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒØ§Ø´ Ù…Ø¹ÙŠÙ†"""
        return self._caches.get(cache_type, self._caches["default"])

    # === Default Cache ===
    def get(self, key: str, default=None):
        return self._caches["default"].get(key, default)

    def set(self, key: str, value: Any, timeout: int = 300):
        return self._caches["default"].set(key, value, timeout)

    def delete(self, key: str):
        return self._caches["default"].delete(key)

    # === Query Cache (Ù‚ØµÙŠØ± Ø§Ù„Ù…Ø¯Ù‰) ===
    def get_query(self, key: str, default=None):
        return self._caches["query"].get(f"q:{key}", default)

    def set_query(self, key: str, value: Any, timeout: int = 60):
        return self._caches["query"].set(f"q:{key}", value, timeout)

    def delete_query(self, key: str):
        return self._caches["query"].delete(f"q:{key}")

    # === Stats Cache (Ø·ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ù‰) ===
    def get_stats(self, key: str, default=None):
        return self._caches["stats"].get(f"s:{key}", default)

    def set_stats(self, key: str, value: Any, timeout: int = 600):
        return self._caches["stats"].set(f"s:{key}", value, timeout)

    def delete_stats(self, key: str):
        return self._caches["stats"].delete(f"s:{key}")

    # === Page Cache ===
    def get_page(self, key: str, default=None):
        return self._caches["page"].get(f"p:{key}", default)

    def set_page(self, key: str, value: Any, timeout: int = 300):
        return self._caches["page"].set(f"p:{key}", value, timeout)

    def delete_page(self, key: str):
        return self._caches["page"].delete(f"p:{key}")

    # === Utility Methods ===
    def clear_all(self):
        """Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØ§Ø´"""
        for cache in self._caches.values():
            try:
                cache.clear()
            except Exception as e:
                logger.warning(f"Failed to clear cache: {e}")

    def clear_pattern(self, pattern: str, cache_type: str = "default"):
        """Ù…Ø³Ø­ Ù…ÙØ§ØªÙŠØ­ ØªØ·Ø§Ø¨Ù‚ Ù†Ù…Ø· Ù…Ø¹ÙŠÙ†"""
        cache = self._caches.get(cache_type, self._caches["default"])
        try:
            if hasattr(cache, "delete_pattern"):
                cache.delete_pattern(f"*{pattern}*")
            else:
                # Fallback Ù„Ù„Ù€ caches Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ¯Ø¹Ù… patterns
                logger.warning(f"Cache {cache_type} doesn't support pattern deletion")
        except Exception as e:
            logger.error(f"Failed to clear pattern {pattern}: {e}")

    def get_stats_info(self) -> dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒØ§Ø´"""
        stats = {}
        for name, cache in self._caches.items():
            try:
                if hasattr(cache, "client"):
                    client = cache.client.get_client()
                    if hasattr(client, "info"):
                        info = client.info()
                        stats[name] = {
                            "used_memory": info.get("used_memory_human", "N/A"),
                            "connected_clients": info.get("connected_clients", "N/A"),
                            "keyspace_hits": info.get("keyspace_hits", 0),
                            "keyspace_misses": info.get("keyspace_misses", 0),
                        }
            except Exception:
                stats[name] = {"status": "unavailable"}
        return stats


# Global instance
cache_manager = CacheManager()


# =============================================
# Decorators Ù„Ù„Ù€ Caching
# =============================================


def cache_result(
    timeout: int = 300,
    cache_type: str = "default",
    key_prefix: str = "",
    vary_on: list = None,
):
    """
    Ø¯ÙŠÙƒÙˆØ±ÙŠØªÙˆØ± Ù„ØªØ®Ø²ÙŠÙ† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¯ÙˆØ§Ù„ ÙÙŠ Ø§Ù„ÙƒØ§Ø´

    Args:
        timeout: Ù…Ø¯Ø© Ø§Ù„ÙƒØ§Ø´ Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
        cache_type: Ù†ÙˆØ¹ Ø§Ù„ÙƒØ§Ø´ (default, query, stats, page)
        key_prefix: Ø¨Ø§Ø¯Ø¦Ø© Ø§Ù„Ù…ÙØªØ§Ø­
        vary_on: Ù‚Ø§Ø¦Ù…Ø© Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù„Ù„ØªÙ…ÙŠÙŠØ²

    Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    @cache_result(timeout=600, cache_type='stats', key_prefix='order_stats')
    def get_order_statistics(branch_id, days=30):
        # ... ÙƒÙˆØ¯ Ø¨Ø·ÙŠØ¡ ...
        return result
    """

    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            # Ø¨Ù†Ø§Ø¡ Ù…ÙØªØ§Ø­ Ø§Ù„ÙƒØ§Ø´
            key_parts = [key_prefix or func.__name__]

            # Ø¥Ø¶Ø§ÙØ© vary_on parameters
            if vary_on:
                for param in vary_on:
                    if param in kwargs:
                        key_parts.append(f"{param}:{kwargs[param]}")
            else:
                # Ø¥Ø¶Ø§ÙØ© hash Ù„ÙƒÙ„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
                all_args = str(args) + str(sorted(kwargs.items()))
                args_hash = hashlib.md5(all_args.encode()).hexdigest()[:8]
                key_parts.append(args_hash)

            cache_key = ":".join(key_parts)

            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ù…Ù† Ø§Ù„ÙƒØ§Ø´
            cache = cache_manager.get_cache(cache_type)
            cached_result = cache.get(cache_key)

            if cached_result is not None:
                logger.debug(f"Cache HIT: {cache_key}")
                return cached_result

            # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯Ø§Ù„Ø©
            result = func(*args, **kwargs)

            # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªÙŠØ¬Ø©
            cache.set(cache_key, result, timeout)
            logger.debug(f"Cache SET: {cache_key}")

            return result

        # Ø¥Ø¶Ø§ÙØ© Ø¯Ø§Ù„Ø© Ù„Ø¥Ø¨Ø·Ø§Ù„ Ø§Ù„ÙƒØ§Ø´
        def invalidate(*args, **kwargs):
            key_parts = [key_prefix or func.__name__]
            if args or kwargs:
                all_args = str(args) + str(sorted(kwargs.items()))
                args_hash = hashlib.md5(all_args.encode()).hexdigest()[:8]
                key_parts.append(args_hash)
            cache_key = ":".join(key_parts)
            cache_manager.get_cache(cache_type).delete(cache_key)

        wrapper.invalidate = invalidate
        return wrapper

    return decorator


def cache_queryset(timeout: int = 60, key_prefix: str = "", evaluate: bool = True):
    """
    Ø¯ÙŠÙƒÙˆØ±ÙŠØªÙˆØ± Ù„ØªØ®Ø²ÙŠÙ† Ù†ØªØ§Ø¦Ø¬ QuerySets

    Args:
        timeout: Ù…Ø¯Ø© Ø§Ù„ÙƒØ§Ø´
        key_prefix: Ø¨Ø§Ø¯Ø¦Ø© Ø§Ù„Ù…ÙØªØ§Ø­
        evaluate: ØªØ­ÙˆÙŠÙ„ QuerySet Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù‚Ø¨Ù„ Ø§Ù„ØªØ®Ø²ÙŠÙ†
    """

    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            key_parts = [key_prefix or func.__name__]
            all_args = str(args) + str(sorted(kwargs.items()))
            key_parts.append(hashlib.md5(all_args.encode()).hexdigest()[:8])
            cache_key = ":".join(key_parts)

            cached = cache_manager.get_query(cache_key)
            if cached is not None:
                return cached

            result = func(*args, **kwargs)

            # ØªØ­ÙˆÙŠÙ„ QuerySet Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù„Ù„ØªØ®Ø²ÙŠÙ†
            if evaluate and hasattr(result, "__iter__") and hasattr(result, "model"):
                result = list(result)

            cache_manager.set_query(cache_key, result, timeout)
            return result

        return wrapper

    return decorator


# =============================================
# Health Check Ù„Ù„Ù€ Redis
# =============================================


def check_redis_health() -> dict:
    """
    ÙØ­Øµ ØµØ­Ø© Ø§ØªØµØ§Ù„ Redis

    Returns:
        dict Ù…Ø¹ Ø­Ø§Ù„Ø© ÙƒÙ„ cache
    """
    results = {}

    for cache_name in ["default", "query", "stats", "page", "session"]:
        try:
            cache = caches.get(cache_name, caches["default"])

            # Ø§Ø®ØªØ¨Ø§Ø± set/get
            test_key = f"health_check_{cache_name}"
            cache.set(test_key, "OK", 10)
            value = cache.get(test_key)
            cache.delete(test_key)

            if value == "OK":
                results[cache_name] = {"status": "healthy", "message": "Connected"}
            else:
                results[cache_name] = {
                    "status": "degraded",
                    "message": "Set/Get mismatch",
                }

        except Exception as e:
            results[cache_name] = {"status": "unhealthy", "message": str(e)}

    return results


def get_redis_info() -> dict:
    """
    Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙØµÙŠÙ„ÙŠØ© Ø¹Ù† Redis
    """
    try:
        from django_redis import get_redis_connection

        conn = get_redis_connection("default")
        info = conn.info()

        return {
            "version": info.get("redis_version"),
            "used_memory": info.get("used_memory_human"),
            "used_memory_peak": info.get("used_memory_peak_human"),
            "connected_clients": info.get("connected_clients"),
            "total_connections_received": info.get("total_connections_received"),
            "total_commands_processed": info.get("total_commands_processed"),
            "keyspace_hits": info.get("keyspace_hits"),
            "keyspace_misses": info.get("keyspace_misses"),
            "hit_rate": round(
                info.get("keyspace_hits", 0)
                / max(info.get("keyspace_hits", 0) + info.get("keyspace_misses", 0), 1)
                * 100,
                2,
            ),
            "uptime_in_days": info.get("uptime_in_days"),
        }
    except Exception as e:
        return {"error": str(e)}


# =============================================
# Exports
# =============================================

__all__ = [
    "REDIS_OPTIMIZED_CONFIG",
    "CacheManager",
    "cache_manager",
    "cache_result",
    "cache_queryset",
    "check_redis_health",
    "get_redis_info",
]
