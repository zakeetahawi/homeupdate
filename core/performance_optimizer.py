"""
ðŸš€ Ù†Ø¸Ø§Ù… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø´Ø§Ù…Ù„ - Performance Optimizer
ØªØ³Ø±ÙŠØ¹ 1000% Ù„Ù„Ù†Ø¸Ø§Ù…

Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:
1. Smart Query Caching - ÙƒØ§Ø´ Ø°ÙƒÙŠ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
2. Query Optimizer - ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
3. Connection Pool Manager - Ø¥Ø¯Ø§Ø±Ø© Ø§ØªØµØ§Ù„Ø§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
4. Response Compression - Ø¶ØºØ· Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª
"""

import functools
import hashlib
import logging
import time
from typing import Any, Callable, Dict, List, Optional, Tuple

from django.conf import settings
from django.core.cache import cache, caches
from django.db import connection, reset_queries
from django.db.models import Prefetch, QuerySet
from django.http import HttpRequest, HttpResponse

logger = logging.getLogger("performance")


# =============================================
# 1. Smart Query Cache Decorator
# =============================================


def smart_cache(timeout: int = 300, key_prefix: str = "", vary_on: List[str] = None):
    """
    Ø¯ÙŠÙƒÙˆØ±ÙŠØªÙˆØ± Ù„Ù„ÙƒØ§Ø´ Ø§Ù„Ø°ÙƒÙŠ - ÙŠØ®Ø²Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¯ÙˆØ§Ù„/Views

    Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:
    - ÙƒØ§Ø´ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù†ØªØ§Ø¦Ø¬
    - Ù…ÙØ§ØªÙŠØ­ Ø°ÙƒÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
    - Ø¯Ø¹Ù… vary_on Ù„Ù„ØªÙ…ÙŠÙŠØ² Ø¨ÙŠÙ† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†/Ø§Ù„ÙØ±ÙˆØ¹
    - invalidation ØªÙ„Ù‚Ø§Ø¦ÙŠ

    Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    @smart_cache(timeout=600, key_prefix='my_view', vary_on=['user_id', 'branch_id'])
    def my_view(request):
        ...
    """

    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ request Ù…Ù† args Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            request = None
            for arg in args:
                if isinstance(arg, HttpRequest):
                    request = arg
                    break

            # Ø¨Ù†Ø§Ø¡ Ù…ÙØªØ§Ø­ Ø§Ù„ÙƒØ§Ø´
            cache_key_parts = [key_prefix or func.__name__]

            # Ø¥Ø¶Ø§ÙØ© vary_on parameters
            if vary_on and request:
                for param in vary_on:
                    if (
                        param == "user_id"
                        and hasattr(request, "user")
                        and request.user.is_authenticated
                    ):
                        cache_key_parts.append(f"user:{request.user.id}")
                    elif param == "branch_id" and hasattr(request.user, "branch_id"):
                        cache_key_parts.append(f"branch:{request.user.branch_id}")
                    elif param in kwargs:
                        cache_key_parts.append(f"{param}:{kwargs[param]}")
                    elif request and hasattr(request, "GET") and param in request.GET:
                        cache_key_parts.append(f"{param}:{request.GET[param]}")

            # Ø¥Ø¶Ø§ÙØ© hash Ù„Ù„Ù€ kwargs
            if kwargs:
                kwargs_hash = hashlib.md5(
                    str(sorted(kwargs.items())).encode()
                ).hexdigest()[:8]
                cache_key_parts.append(kwargs_hash)

            cache_key = ":".join(cache_key_parts)

            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ù…Ù† Ø§Ù„ÙƒØ§Ø´
            cached_result = cache.get(cache_key)
            if cached_result is not None:
                logger.debug(f"Cache HIT: {cache_key}")
                return cached_result

            # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯Ø§Ù„Ø© ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªÙŠØ¬Ø©
            result = func(*args, **kwargs)

            # ØªØ®Ø²ÙŠÙ† ÙÙŠ Ø§Ù„ÙƒØ§Ø´ (Ù„Ø§ Ù†Ø®Ø²Ù† HttpResponse Ù…Ø¨Ø§Ø´Ø±Ø©)
            if not isinstance(result, HttpResponse):
                cache.set(cache_key, result, timeout)
                logger.debug(f"Cache SET: {cache_key}")

            return result

        # Ø¥Ø¶Ø§ÙØ© Ø¯Ø§Ù„Ø© Ù„Ø¥Ø¨Ø·Ø§Ù„ Ø§Ù„ÙƒØ§Ø´
        def invalidate(*args, **kwargs):
            cache_key_parts = [key_prefix or func.__name__]
            if kwargs:
                kwargs_hash = hashlib.md5(
                    str(sorted(kwargs.items())).encode()
                ).hexdigest()[:8]
                cache_key_parts.append(kwargs_hash)
            cache_key = ":".join(cache_key_parts)
            cache.delete(cache_key)
            logger.debug(f"Cache INVALIDATED: {cache_key}")

        wrapper.invalidate = invalidate
        return wrapper

    return decorator


# =============================================
# 2. QuerySet Optimizer
# =============================================


class QuerySetOptimizer:
    """
    Ù…Ø­Ø³Ù‘Ù† QuerySet - ÙŠØ¶ÙŠÙ select_related Ùˆ prefetch_related ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹

    Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    optimizer = QuerySetOptimizer()
    optimized_qs = optimizer.optimize(Order.objects.all())
    """

    # Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© Ù„Ù„Ù€ Models
    RELATION_MAP = {
        "Order": {
            "select_related": ["customer", "salesperson", "branch", "created_by"],
            "prefetch_related": [
                "items",
                "items__product",
                "payments",
                "manufacturing_orders",
            ],
            "only_fields": [
                "id",
                "order_number",
                "order_date",
                "status",
                "order_status",
                "tracking_status",
                "total_amount",
                "paid_amount",
                "final_price",
                "customer__id",
                "customer__name",
                "customer__phone",
                "salesperson__id",
                "salesperson__name",
                "branch__id",
                "branch__name",
            ],
        },
        "InstallationSchedule": {
            "select_related": [
                "order",
                "order__customer",
                "order__branch",
                "order__salesperson",
                "team",
                "team__driver",
            ],
            "prefetch_related": ["team__technicians"],
            "only_fields": [
                "id",
                "status",
                "scheduled_date",
                "created_at",
                "location_type",
                "order__id",
                "order__order_number",
                "order__contract_number",
                "order__customer__id",
                "order__customer__name",
                "order__customer__phone",
                "team__id",
                "team__name",
            ],
        },
        "ManufacturingOrder": {
            "select_related": [
                "order",
                "order__customer",
                "order__branch",
                "order__salesperson",
                "production_line",
            ],
            "prefetch_related": ["items", "items__order_item"],
            "only_fields": [
                "id",
                "status",
                "order_type",
                "created_at",
                "expected_delivery_date",
                "order__id",
                "order__order_number",
                "order__customer__id",
                "order__customer__name",
                "production_line__id",
                "production_line__name",
            ],
        },
        "Product": {
            "select_related": ["category"],
            "prefetch_related": [],
            "only_fields": [
                "id",
                "name",
                "code",
                "price",
                "currency",
                "unit",
                "category__id",
                "category__name",
            ],
        },
    }

    def optimize(self, queryset: QuerySet, include_only: bool = True) -> QuerySet:
        """
        ØªØ­Ø³ÙŠÙ† QuerySet ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Model

        Args:
            queryset: QuerySet Ù„Ù„ØªØ­Ø³ÙŠÙ†
            include_only: Ù‡Ù„ Ù†Ø¶ÙŠÙ only() Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ù‚ÙˆÙ„

        Returns:
            QuerySet Ù…Ø­Ø³Ù‘Ù†
        """
        model_name = queryset.model.__name__

        if model_name not in self.RELATION_MAP:
            return queryset

        relations = self.RELATION_MAP[model_name]

        # Ø¥Ø¶Ø§ÙØ© select_related
        if relations.get("select_related"):
            queryset = queryset.select_related(*relations["select_related"])

        # Ø¥Ø¶Ø§ÙØ© prefetch_related
        if relations.get("prefetch_related"):
            queryset = queryset.prefetch_related(*relations["prefetch_related"])

        # Ø¥Ø¶Ø§ÙØ© only() Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ù‚ÙˆÙ„
        if include_only and relations.get("only_fields"):
            try:
                queryset = queryset.only(*relations["only_fields"])
            except Exception:
                # Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ù‚Ø¯ Ù„Ø§ ØªÙƒÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯Ø© - ØªØ¬Ø§Ù‡Ù„
                pass

        return queryset

    def optimize_for_list(self, queryset: QuerySet) -> QuerySet:
        """ØªØ­Ø³ÙŠÙ† Ù„Ù„Ø¹Ø±Ø¶ ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© (Ø£Ù‚Ù„ Ø¨ÙŠØ§Ù†Ø§Øª)"""
        return self.optimize(queryset, include_only=True)

    def optimize_for_detail(self, queryset: QuerySet) -> QuerySet:
        """ØªØ­Ø³ÙŠÙ† Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ (ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)"""
        return self.optimize(queryset, include_only=False)


# =============================================
# 3. Query Counter & Logger
# =============================================


class QueryCounter:
    """
    Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª - Ù„ØªØªØ¨Ø¹ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡

    Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    with QueryCounter() as counter:
        # Ø§Ù„ÙƒÙˆØ¯ Ù‡Ù†Ø§
        pass
    print(f"Queries: {counter.count}, Time: {counter.time}ms")
    """

    def __init__(self, log_queries: bool = False, warn_threshold: int = 20):
        self.log_queries = log_queries
        self.warn_threshold = warn_threshold
        self.count = 0
        self.time = 0
        self.queries = []
        self._start_queries = 0
        self._start_time = 0

    def __enter__(self):
        # ØªÙØ¹ÙŠÙ„ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
        if not settings.DEBUG:
            connection.force_debug_cursor = True
        reset_queries()
        self._start_queries = len(connection.queries)
        self._start_time = time.time()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.time = (time.time() - self._start_time) * 1000
        self.queries = connection.queries[self._start_queries :]
        self.count = len(self.queries)

        # ØªØ­Ø°ÙŠØ± Ø¥Ø°Ø§ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­
        if self.count > self.warn_threshold:
            logger.warning(
                f"HIGH_QUERY_COUNT: {self.count} queries in {self.time:.0f}ms"
            )

        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¥Ø°Ø§ Ù…Ø·Ù„ÙˆØ¨
        if self.log_queries:
            for i, query in enumerate(self.queries, 1):
                logger.debug(f"Query {i}: {query['sql'][:200]}...")

        # Ø¥Ø¹Ø§Ø¯Ø© Ø¶Ø¨Ø· debug cursor
        if not settings.DEBUG:
            connection.force_debug_cursor = False

        return False

    def get_slow_queries(self, threshold_ms: float = 100) -> List[Dict]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø·ÙŠØ¦Ø©"""
        return [
            q for q in self.queries if float(q.get("time", 0)) * 1000 > threshold_ms
        ]

    def get_duplicate_queries(self) -> Dict[str, int]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        query_counts = {}
        for q in self.queries:
            sql_hash = hashlib.md5(q["sql"].encode()).hexdigest()
            query_counts[sql_hash] = query_counts.get(sql_hash, 0) + 1
        return {k: v for k, v in query_counts.items() if v > 1}


# =============================================
# 4. Bulk Operations Helper
# =============================================


class BulkOperations:
    """
    Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© - Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø­ÙØ¸ ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ«

    Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    bulk_ops = BulkOperations()
    bulk_ops.queue_update(order, ['status', 'updated_at'])
    bulk_ops.execute()
    """

    def __init__(self, batch_size: int = 100):
        self.batch_size = batch_size
        self._create_queue = {}  # {model_class: [instances]}
        self._update_queue = {}  # {model_class: {fields: [instances]}}

    def queue_create(self, instance):
        """Ø¥Ø¶Ø§ÙØ© instance Ù„Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù…Ø¹"""
        model_class = instance.__class__
        if model_class not in self._create_queue:
            self._create_queue[model_class] = []
        self._create_queue[model_class].append(instance)

    def queue_update(self, instance, fields: List[str]):
        """Ø¥Ø¶Ø§ÙØ© instance Ù„Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø¬Ù…Ø¹"""
        model_class = instance.__class__
        fields_key = tuple(sorted(fields))

        if model_class not in self._update_queue:
            self._update_queue[model_class] = {}
        if fields_key not in self._update_queue[model_class]:
            self._update_queue[model_class][fields_key] = []

        self._update_queue[model_class][fields_key].append(instance)

    def execute(self) -> Dict[str, int]:
        """ØªÙ†ÙÙŠØ° Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©"""
        results = {"created": 0, "updated": 0}

        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù…Ø¹
        for model_class, instances in self._create_queue.items():
            for i in range(0, len(instances), self.batch_size):
                batch = instances[i : i + self.batch_size]
                model_class.objects.bulk_create(batch, ignore_conflicts=True)
                results["created"] += len(batch)

        # ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø¬Ù…Ø¹
        for model_class, fields_dict in self._update_queue.items():
            for fields, instances in fields_dict.items():
                for i in range(0, len(instances), self.batch_size):
                    batch = instances[i : i + self.batch_size]
                    model_class.objects.bulk_update(batch, list(fields))
                    results["updated"] += len(batch)

        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…
        self._create_queue.clear()
        self._update_queue.clear()

        return results


# =============================================
# 5. Response Cache Helper
# =============================================


def cache_page_smart(
    timeout: int = 300, cache_name: str = "default", key_func: Callable = None
):
    """
    Ø¯ÙŠÙƒÙˆØ±ÙŠØªÙˆØ± ÙƒØ§Ø´ ØµÙØ­Ø§Øª Ø°ÙƒÙŠ - Ø£ÙØ¶Ù„ Ù…Ù† cache_page Ø§Ù„Ø¹Ø§Ø¯ÙŠ

    Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:
    - ÙŠØªØ¬Ø§Ù‡Ù„ POST requests
    - ÙŠØªØ¬Ø§Ù‡Ù„ authenticated users Ø¨Ø´ÙƒÙ„ Ø§Ø®ØªÙŠØ§Ø±ÙŠ
    - Ù…ÙØ§ØªÙŠØ­ Ø°ÙƒÙŠØ©

    Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    @cache_page_smart(timeout=600)
    def my_view(request):
        ...
    """

    def decorator(view_func: Callable) -> Callable:
        @functools.wraps(view_func)
        def wrapper(request: HttpRequest, *args, **kwargs) -> HttpResponse:
            # Ù„Ø§ Ù†Ø®Ø²Ù† POST requests
            if request.method != "GET":
                return view_func(request, *args, **kwargs)

            # Ø¨Ù†Ø§Ø¡ Ù…ÙØªØ§Ø­ Ø§Ù„ÙƒØ§Ø´
            if key_func:
                cache_key = key_func(request, *args, **kwargs)
            else:
                cache_key = f"page:{request.path}:{request.GET.urlencode()}"
                if request.user.is_authenticated:
                    cache_key += f":user:{request.user.id}"

            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ù…Ù† Ø§Ù„ÙƒØ§Ø´
            cache_backend = caches[cache_name]
            cached_response = cache_backend.get(cache_key)
            if cached_response is not None:
                return cached_response

            # ØªÙ†ÙÙŠØ° Ø§Ù„Ù€ view
            response = view_func(request, *args, **kwargs)

            # ØªØ®Ø²ÙŠÙ† ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù†Ø§Ø¬Ø­Ø©
            if response.status_code == 200:
                cache_backend.set(cache_key, response, timeout)

            return response

        return wrapper

    return decorator


# =============================================
# 6. Database Connection Health Check
# =============================================


def check_db_connection() -> Tuple[bool, str]:
    """
    ÙØ­Øµ ØµØ­Ø© Ø§ØªØµØ§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

    Returns:
        (is_healthy, message)
    """
    try:
        start_time = time.time()
        with connection.cursor() as cursor:
            cursor.execute("SELECT 1")
            cursor.fetchone()
        elapsed = (time.time() - start_time) * 1000

        if elapsed > 100:
            return True, f"Database responding slowly: {elapsed:.0f}ms"
        return True, f"Database healthy: {elapsed:.0f}ms"

    except Exception as e:
        return False, f"Database error: {str(e)}"


def get_db_stats() -> Dict[str, Any]:
    """
    Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    """
    stats = {}

    try:
        with connection.cursor() as cursor:
            # Ø¹Ø¯Ø¯ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©
            cursor.execute(
                """
                SELECT count(*) FROM pg_stat_activity 
                WHERE state = 'active' AND datname = current_database()
            """
            )
            stats["active_connections"] = cursor.fetchone()[0]

            # Ø­Ø¬Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            cursor.execute(
                """
                SELECT pg_size_pretty(pg_database_size(current_database()))
            """
            )
            stats["database_size"] = cursor.fetchone()[0]

            # Ø£Ø¨Ø·Ø£ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª (Ø¥Ø°Ø§ ÙƒØ§Ù† pg_stat_statements Ù…ÙØ¹Ù‘Ù„)
            try:
                cursor.execute(
                    """
                    SELECT query, mean_exec_time, calls
                    FROM pg_stat_statements
                    ORDER BY mean_exec_time DESC
                    LIMIT 5
                """
                )
                stats["slow_queries"] = cursor.fetchall()
            except Exception:
                stats["slow_queries"] = []

    except Exception as e:
        stats["error"] = str(e)

    return stats


# =============================================
# 7. Performance Metrics Collector
# =============================================


class PerformanceMetrics:
    """
    Ø¬Ø§Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ - Ù„Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„
    """

    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._metrics = {}
        return cls._instance

    def record(self, name: str, value: float, tags: Dict[str, str] = None):
        """ØªØ³Ø¬ÙŠÙ„ Ù‚ÙŠÙ…Ø©"""
        key = f"{name}:{tags}" if tags else name
        if key not in self._metrics:
            self._metrics[key] = []
        self._metrics[key].append(
            {"value": value, "timestamp": time.time(), "tags": tags or {}}
        )

        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 1000 Ù‚ÙŠÙ…Ø© ÙÙ‚Ø·
        if len(self._metrics[key]) > 1000:
            self._metrics[key] = self._metrics[key][-1000:]

    def get_avg(self, name: str, tags: Dict[str, str] = None) -> float:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØªÙˆØ³Ø·"""
        key = f"{name}:{tags}" if tags else name
        if key not in self._metrics or not self._metrics[key]:
            return 0
        values = [m["value"] for m in self._metrics[key]]
        return sum(values) / len(values)

    def get_percentile(
        self, name: str, percentile: float, tags: Dict[str, str] = None
    ) -> float:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ©"""
        key = f"{name}:{tags}" if tags else name
        if key not in self._metrics or not self._metrics[key]:
            return 0
        values = sorted([m["value"] for m in self._metrics[key]])
        index = int(len(values) * percentile / 100)
        return values[min(index, len(values) - 1)]

    def get_summary(self) -> Dict[str, Dict[str, float]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ø®Øµ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""
        summary = {}
        for key in self._metrics:
            values = [m["value"] for m in self._metrics[key]]
            if values:
                summary[key] = {
                    "count": len(values),
                    "avg": sum(values) / len(values),
                    "min": min(values),
                    "max": max(values),
                    "p95": self.get_percentile(key.split(":")[0], 95),
                }
        return summary


# =============================================
# 8. Initialization & Exports
# =============================================

# Global instances
query_optimizer = QuerySetOptimizer()
performance_metrics = PerformanceMetrics()

# Export all
__all__ = [
    "smart_cache",
    "cache_page_smart",
    "QuerySetOptimizer",
    "QueryCounter",
    "BulkOperations",
    "check_db_connection",
    "get_db_stats",
    "PerformanceMetrics",
    "query_optimizer",
    "performance_metrics",
]
